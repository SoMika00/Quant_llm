# Guide Pratique & Strat√©gique (2025) : Dimensionnement, D√©ploiement et Quantization de LLM

**Statut :** Valid√©  
**Cible :** Ing√©nieurs MLOps, AI Architects, DevOps  
**Derni√®re mise √† jour :** 2026-02-22  

Ce document synth√©tise des standards et bonnes pratiques pour d√©ployer des LLMs en production, avec un focus sur :
- **Dimensionnement VRAM** (poids + KV-cache + overhead runtime)
- **Quantization** (FP8 / INT8 / INT4) et impacts qualit√©/perf
- **Moteurs d‚Äôinf√©rence** (TensorRT-LLM, vLLM, llama.cpp/GGUF)
- **D√©cisions d‚Äôarchitecture** (latence, throughput, co√ªts, contraintes mat√©rielles)

> Note GitHub : le LaTeX inline peut casser sur certains `_` (ex: dans `\text{...}` / `\texttt{...}`) et produire `_' allowed only in math mode`.  
> Les formules critiques sont donc donn√©es en **code** pour un rendu 100% fiable.

---

## Table des mati√®res
- [0. Notations et hypoth√®ses](#0-notations-et-hypoth√®ses)
- [1. Fondations : m√©moire d‚Äôun LLM en inf√©rence](#1-fondations--m√©moire-dun-llm-en-inf√©rence)
- [2. Dimensionnement VRAM : combien gagne-t-on ?](#2-dimensionnement-vram--combien-gagne-t-on-)
- [3. M√©thodes de quantization cl√©s](#3-m√©thodes-de-quantization-cl√©s)
- [4. Comparatif des moteurs d‚Äôinf√©rence](#4-comparatif-des-moteurs-dinf√©rence)
- [5. Arbre de d√©cision : quelle strat√©gie d√©ployer ?](#5-arbre-de-d√©cision--quelle-strat√©gie-d√©ployer-)
- [6. Production : observabilit√©, SLIs, licences](#6-production--observabilit√©-slis-licences)
- [7. Cheat-sheet : commandes de d√©ploiement (avec explications)](#7-cheat-sheet--commandes-de-d√©ploiement-avec-explications)
- [8. R√©f√©rences](#8-r√©f√©rences)
- [Annexes : mini-calculateur VRAM/KV-cache](#annexes--mini-calculateur-vramkv-cache)

---

## 0. Notations et hypoth√®ses

### Unit√©s
- **GiB** (gibibyte) = `1024^3` bytes (plus pr√©cis que ‚ÄúGB‚Äù)
- Les ‚ÄúGo‚Äù affich√©s par les GPU vendors sont parfois en base 10 ‚Üí garde une marge.

### Notations
- `B` = batch_size (requ√™tes simultan√©es effectives)
- `S` = sequence_length (tokens contexte total pris en charge : prompt + g√©n√©ration)
- `L` = num_layers
- `H` = hidden_size
- `num_heads` = nombre de t√™tes attention (Query heads)
- `num_kv_heads` = nombre de t√™tes K/V (MQA/GQA)
- `gqa_factor = num_heads / num_kv_heads`
- `bytes_kv` = bytes par √©l√©ment du KV-cache (ex: FP16=2, FP8=1)

---

## 1. Fondations : m√©moire d‚Äôun LLM en inf√©rence

En production, la VRAM est principalement consomm√©e par trois blocs :

1. **Poids (Weights)**  
   Param√®tres du mod√®le (ce qui ‚Äúscale‚Äù le plus avec le nombre de param√®tres).
2. **KV-Cache (Key/Value Cache)**  
   Stockage des cl√©s/valeurs d‚Äôattention pour √©viter de recalculer l‚Äôhistorique (auto-r√©gression).  
   üëâ Cro√Æt ~lin√©airement avec **S** et **B**.
3. **Overhead runtime** (souvent sous-estim√©)  
   CUDA context, buffers, allocations du moteur, fragmentation, graph capture, etc.

> Les **activations** en pur ‚Äúdecode‚Äù sont souvent moins dominantes que le KV-cache, mais la phase **prefill** peut g√©n√©rer des pics selon le moteur et la config.

### 1.1 Formule pratique du KV-cache (GQA/MQA inclus)

Formule ‚Äúlisible‚Äù (approximation utile pour capacity planning) :

`KV_bytes ‚âà B √ó S √ó L √ó 2 √ó (H / gqa_factor) √ó bytes_kv`

- `2` = K et V  
- `(H / gqa_factor)` est √©quivalent √† `num_kv_heads √ó head_dim`

Version ‚Äúexplicite‚Äù :

`KV_bytes ‚âà B √ó S √ó L √ó 2 √ó (num_kv_heads √ó head_dim) √ó bytes_kv`

#### Exemple (ordre de grandeur)
Sur un 70B typique (GQA), passer le KV-cache de FP16 ‚Üí FP8 divise **‚âà par 2** l‚Äôempreinte KV-cache, ce qui permet soit :
- plus de contexte (`S`),
- plus de concurrence (`B`),
- moins de risques d‚ÄôOOM.

---

### 1.2 PagedAttention : pourquoi √ßa change tout

**PagedAttention** (vLLM) d√©coupe le KV-cache en blocs ‚Äúpagin√©s‚Äù, ce qui r√©duit les pertes dues √† la r√©servation + fragmentation.  
Dans le papier vLLM, la **KV cache usage** mesur√©e monte jusqu‚Äô√† ~**96.3%** pour vLLM, tandis que des baselines (Orca variants) restent beaucoup plus bas selon les sc√©narios (r√©servation/fragmentation).  
üëâ R√©sultat : plus de requ√™tes ‚Äúin-flight‚Äù √† VRAM √©gale, et meilleure tenue sous charge.

---

## 2. Dimensionnement VRAM : combien gagne-t-on ?

### 2.1 R√®gle d‚Äôor (poids)
Approximation poids seuls :

`Weights_bytes ‚âà num_params √ó bytes_per_weight`

- FP16/BF16 : 2 bytes
- FP8/INT8 : 1 byte (en pratique : +scales/metadata selon m√©thode)
- INT4 : 0.5 byte (en pratique : +scales/packing + parfois ‚Äúoutliers‚Äù en FP16)

> Toujours ajouter une **marge** (souvent 10‚Äì25%) pour l‚Äôoverhead moteur + KV-cache selon ton S et B.

---

### 2.2 √âtude de cas (valeurs indicatives)

Hypoth√®ses pour ‚ÄúVRAM totale‚Äù :
- KV-cache activ√© (taille d√©pendante de `S` et `B`)
- Overhead moteur inclus en ‚Äúmarge‚Äù
- Les chiffres restent des **ordres de grandeur** : la v√©rit√© d√©pend de `max_model_len`, batching, backend attention, kernels, etc.

| Mod√®le cible | Format | VRAM Poids seuls (‚âà) | VRAM Totale (indicatif) | GPU minimum ‚Äúconfort‚Äù | Gain vs FP16 |
|---|---:|---:|---:|---|---:|
| 8B | FP16 | ~16 GiB | ~18‚Äì22 GiB | RTX 3090/4090 (24GB) / A10 | r√©f |
| 8B | FP8 / INT8 | ~8 GiB | ~10‚Äì14 GiB | 16GB (selon contexte) | ~-50% |
| 8B | INT4 (AWQ/GGUF) | ~4 GiB | ~6‚Äì10 GiB | 12GB possible | ~-75% |
| 70B | FP16 | ~140 GiB | ~150‚Äì180 GiB | 2√ó80GB (TP) | r√©f |
| 70B | FP8 / INT8 | ~70 GiB | ~75‚Äì110 GiB | 1√ó80GB **si** contexte/batch ma√Ætris√©s | ~-50% |
| 70B | INT4 (AWQ/GGUF) | ~35 GiB | ~40‚Äì70 GiB | 48GB ou 2√ó24GB (TP) | ~-75% |

---

### 2.3 Le compromis : VRAM vs Vitesse vs Qualit√©

#### FP8 / INT8 : ‚Äúsweet spot‚Äù production
- **M√©moire** : ~2√ó moins de VRAM poids (th√©orique)
- **Perf** : sur mat√©riel support√©, vLLM indique jusqu‚Äô√† **~1.6√ó** de throughput avec impact minimal sur l‚Äôaccuracy selon mod√®les/t√¢ches.  
- **KV-cache FP8** : gros levier sur contexte et concurrence.

#### INT4 : ‚Äúdensit√© / co√ªt‚Äù
- **M√©moire** : √©norme compression
- **Qualit√©** : d√©pend du mod√®le + m√©thode (AWQ souvent tr√®s bon ratio, GPTQ variable)
- **Perf** : parfois limit√©e par kernels / d√©quant / m√©moire plut√¥t que compute pur

---

## 3. M√©thodes de quantization cl√©s

Trois familles PTQ dominent l‚Äô√©cosyst√®me :

- **SmoothQuant (INT8 W8A8)**  
  R√©duit l‚Äôeffet des outliers en redistribuant l‚Äôamplitude activations‚Üîpoids via rescaling.  
  Bon compromis quand tu veux INT8 stable sans retraining.

- **AWQ (INT4)**  
  ‚ÄúProtection‚Äù d‚Äôune petite partie des poids les plus sensibles, quantization du reste en 4-bit.  
  Tr√®s populaire pour servir des mod√®les lourds avec faible VRAM.

- **GPTQ (INT3/INT4)**  
  One-shot par blocs (approx Hessienne) pour compenser l‚Äôerreur de quantization.  
  Tr√®s utilis√© c√¥t√© open-source.

---

## 4. Comparatif des moteurs d‚Äôinf√©rence

### A) TensorRT-LLM (NVIDIA)
Runtime/stack ultra-optimis√© GPU NVIDIA (latence et d√©bit max possibles).
- **Points forts** : FP8, KV-cache FP8, kernels optimis√©s, serveur compatible OpenAI, endpoint metrics.
- **Tradeoffs** : d√©pendance CUDA/NVIDIA, phase build/engine.

### B) vLLM (Open-Source)
Standard industriel ‚Äúserving‚Äù pour LLM (PagedAttention + gros throughput sous charge).
- **Points forts** : PagedAttention, config riche KV-cache, quantization FP8/INT8/INT4, API OpenAI-compatible.
- **Tradeoffs** : TRT-LLM peut garder l‚Äôavantage sur certains profils latence extr√™me.

### C) llama.cpp / GGUF (CPU & Edge)
Ex√©cution locale / edge, tr√®s pratique pour environnements contraints.
- **Points forts** : portable, large palette de quantizations (Q4_K_M, Q8_0, etc.)
- **Tradeoffs** : pas fait pour exploiter un datacenter GPU au maximum.

---

## 5. Arbre de d√©cision : quelle strat√©gie d√©ployer ?

1. **H100 / Hopper : perf + qualit√©**
   - **Poids** : FP8 (W8A8) si possible
   - **KV-cache** : FP8
   - **Moteur** : TensorRT-LLM ou vLLM
   - **Quand** : prod exigeante, gros trafic, contexte long

2. **A100 / Ampere & parc h√©t√©rog√®ne : robustesse**
   - **Poids** : INT8 (SmoothQuant) souvent safe
   - **KV-cache** : FP8 si support√©, sinon BF16/FP16
   - **Moteur** : vLLM
   - **Quand** : large compat, d√©ploiements rapides, bon ratio perf/co√ªt

3. **Budget / haute densit√©**
   - **Poids** : INT4 (AWQ / GGUF)
   - **KV-cache** : FP8 si possible
   - **Moteur** : vLLM (serve) ou llama.cpp (local/edge)
   - **Quand** : chatbots internes, workloads tol√©rants √† l√©g√®re baisse qualit√©

---

## 6. Production : observabilit√©, SLIs, licences

### 6.1 Monitoring (exemples de SLIs)
| M√©trique | Seuil d‚Äôalerte (exemple) | Actions typiques |
|---|---:|---|
| **TTFT** (time-to-first-token) p99 | > 500 ms | scale-out, profiling kernels, r√©duire `max_model_len` |
| **Tokens/s** (d√©bit) | chute > 20% | v√©rifier batching, contention GPU, throttling |
| **OOM rate** | > 0.5% req | r√©duire `max_model_len`, baisser `B`, KV-cache FP8 |
| **KV-cache waste / residency** | d√©rive | revoir config cache, prefix caching, warmup |

### 6.2 Licensing (rappel)
La quantization est une transformation technique : **elle ne change pas la licence** du mod√®le.
- Un mod√®le fusionn√©/merg√© h√©rite des contraintes amont.
- Si usage commercial interdit sur le mod√®le base ‚Üí interdit aussi sur versions quantized.
- En cas de blocage : partager les **scripts** de repro plut√¥t que les poids.

---

## 7. Cheat-sheet : commandes de d√©ploiement (avec explications)

### 7.1 vLLM : servir un mod√®le avec FP8 + KV-cache FP8

**Pourquoi** : rapide √† mettre en prod, tr√®s bon throughput sous charge, configuration KV-cache riche.

<details>
<summary><strong>Commande vLLM (FP8 + KV FP8)</strong></summary>

```bash
vllm serve mistralai/Mistral-7B-Instruct \
  --quantization fp8 \
  --kv-cache-dtype fp8 \
  --max-model-len 16384

Notes importantes

--kv-cache-dtype fp8 r√©duit fortement la m√©moire KV (utile pour long contexte / plus de concurrence).

Si ton checkpoint ne contient pas d‚Äô√©chelles KV adapt√©es, tu peux explorer :

--calculate-kv-scales (warmup/calibration)

ou calibration dataset (recommand√© en prod).

</details>
7.2 TensorRT-LLM : quantize ‚Üí build engine ‚Üí serve (OpenAI-compatible)

Pourquoi : latence et d√©bit max sur GPU NVIDIA, tr√®s pertinent sur H100/H200.

<details> <summary><strong>√âtape A ‚Äî (Optionnel) Quantization Toolkit ‚Üí checkpoint TensorRT-LLM</strong></summary>
# Exemple FP8 + KV-cache FP8 (calibration requise)
python examples/quantization/quantize.py \
  --model_dir $MODEL_HF_DIR \
  --qformat fp8 \
  --kv_cache_dtype fp8 \
  --output_dir $TRTLLM_CKPT_DIR

Le checkpoint export√© peut ensuite √™tre utilis√© directement par trtllm-build.

</details> <details> <summary><strong>√âtape B ‚Äî Build de l‚Äôengine</strong></summary>
trtllm-build \
  --checkpoint_dir $TRTLLM_CKPT_DIR \
  --output_dir $TRTLLM_ENGINE_DIR
</details> <details> <summary><strong>√âtape C ‚Äî Serving OpenAI-compatible</strong></summary>
trtllm-serve $TRTLLM_ENGINE_DIR --port 8080

Endpoints : /v1/chat/completions, /v1/completions, etc.

Observabilit√© : /metrics, /health, /version.

</details>
8. R√©f√©rences
Papers

SmoothQuant ‚Äî Xiao et al. (2023). SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. ICML.
https://arxiv.org/abs/2211.10438

AWQ ‚Äî Lin et al. (2024). AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. MLSys.
https://arxiv.org/abs/2306.00978

GPTQ ‚Äî Frantar et al. (2023). GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers. ICLR.
https://arxiv.org/abs/2210.17323

LLM.int8() ‚Äî Dettmers et al. (2022). LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale. NeurIPS.
https://arxiv.org/abs/2208.07339

PagedAttention / vLLM ‚Äî Kwon et al. (2023). Efficient Memory Management for Large Language Model Serving with PagedAttention.
https://arxiv.org/abs/2309.06180

Docs / vendors

vLLM FP8 : https://docs.vllm.ai/en/stable/features/quantization/fp8/

vLLM KV-cache FP8 : https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/

TensorRT-LLM docs (build/serve) : https://nvidia.github.io/TensorRT-LLM/

Annexes : mini-calculateur VRAM/KV-cache
def gib(x_bytes: float) -> float:
    return x_bytes / (1024**3)

def weights_gib(num_params: float, bytes_per_weight: float) -> float:
    return gib(num_params * bytes_per_weight)

def kv_cache_gib(B: int, S: int, L: int, H: int, gqa_factor: int, bytes_kv: float) -> float:
    # KV_bytes ‚âà B √ó S √ó L √ó 2 √ó (H / gqa_factor) √ó bytes_kv
    return gib(B * S * L * 2 * (H / gqa_factor) * bytes_kv)

# Exemple quick check :
# 70B FP16 poids seuls ‚âà 70e9 * 2 bytes
print("Weights 70B FP16 ~", weights_gib(70e9, 2), "GiB")

Points ‚Äúsources v√©rifi√©es‚Äù que j‚Äôai align√©s explicitement :
- vLLM FP8 : **2√ó r√©duction m√©moire** et **jusqu‚Äô√† ~1.6√ó throughput** :contentReference[oaicite:2]{index=2}  
- vLLM KV-cache FP8 + options/calibration :contentReference[oaicite:3]{index=3}  
- PagedAttention : am√©lioration d‚Äôutilisation m√©moire KV-cache (paper vLLM) :contentReference[oaicite:4]{index=4}  
- TensorRT-LLM : serveur OpenAI-compatible + `/metrics` :contentReference[oaicite:5]{index=5}  
- TensorRT-LLM : H100 vs A100 (jusqu‚Äô√† 4.6√ó max throughput en FP8 selon leur blog) :contentReference[oaicite:6]{index=6}  
- Probl√®me GitHub / underscores en LaTeX :contentReference[oaicite:7]{index=7}  

Si tu veux, je peux aussi te proposer une **section ‚ÄúBench protocole‚Äù** (comment mesurer TTFT/tok/s proprement + profils latency/throughput) calibr√©e pour H100/A100, mais l√† tu as d√©j√† une version GitHub ‚Äúnickel‚Äù et sourc√©e.
::contentReference[oaicite:8]{index=8}
