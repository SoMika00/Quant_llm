Skip to content
Navigation Menu
SoMika00
Quant_llm

Type / to search
Code
Issues
Pull requests
Actions
Projects
Wiki
Security
Insights
Settings
Commit c4335f0
SoMika00
SoMika00
authored
1 hour ago
Verified
Update Article.md
main
1 parent 
0f11bed
 commit 
c4335f0
File tree
Filter files‚Ä¶
Article.md
1 file changed
+6
-6
lines changed
Search within code
 
‚ÄéArticle.md
+6
-6
Lines changed: 6 additions & 6 deletions


Guide pratique (2025) ‚Äî Quantization LLM sur H100 et alternatives (TRT-LLM, vLLM, GGUF)

Les bases : tenseurs, poids, activations, KV-cache
Un tenseur est un tableau multi-dimensionnel de nombres (scalaires, vecteurs, matrices, etc.). Dans un LLM, les poids (param√®tres du mod√®le) sont stock√©s sur disque puis charg√©s en VRAM sous forme de tenseurs, tandis que les activations d√©signent les r√©sultats interm√©diaires calcul√©s pendant l‚Äôinf√©rence. La m√©moire KV-cache correspond aux tenseurs de cl√©s (Keys) et valeurs (Values) de l‚Äôattention, conserv√©s au fil de la g√©n√©ration. Ce KV-cache acc√©l√®re l‚Äôauto-r√©gression en √©vitant de recalculer tout l‚Äôhistorique √† chaque nouveau token g√©n√©r√©. Sur les serveurs modernes, sa gestion est optimis√©e (ex. Paged Attention qui partitionne ce cache en pages, et in-flight batching qui regroupe des requ√™tes en vol). Par exemple, la biblioth√®que open-source vLLM introduit PagedAttention pour stocker les Keys/Values en blocs non contigus, ce qui r√©duit le gaspillage de m√©moire √† moins de 4% (contre 60‚Äì80% de m√©moire KV perdue dans les syst√®mes classiques) runpod.io . En r√©sum√©, la VRAM requise √† l‚Äôinf√©rence se compose des poids du mod√®le, des activations temporaires, et du KV-cache dont la taille cro√Æt avec la longueur de contexte. Dans la pratique, les poids dominent souvent l‚Äôempreinte m√©moire (environ 65% de la VRAM sur un mod√®le 13B), le KV-cache occupant ~30% (variable selon la longueur de s√©quence) et les activations une part minime ar5iv.labs.arxiv.org . Cela signifie que la taille minimale sur disque est surtout dict√©e par les poids, tandis qu‚Äôen RAM/VRAM l‚Äôusage effectif d√©pend beaucoup du KV-cache pour les contextes longs.

Overflow / Underflow ‚Äì Rappel rapide : un overflow survient quand une valeur d√©passe la plage repr√©sentable par le format num√©rique choisi et devient infinie, tandis qu‚Äôun underflow se produit lorsque la valeur est trop petite pour √™tre repr√©sent√©e (elle est alors arrondie √† z√©ro). Les formats √† ¬´ petite plage ¬ª (faible dynamique), comme FP8 ou INT4/INT8, n√©cessitent des techniques de calibration pour √©viter ces probl√®mes. Par exemple, la m√©thode SmoothQuant ajuste l‚Äô√©chelle des poids et activations pour ¬´ lisser ¬ª les outliers (valeurs extr√™mes) avant quantization arxiv.org .

Formats num√©riques : FP32, BF16, FP16, FP8, INT8, INT4
FP32 (float 32 bits) : format virgule flottante 32 bits (23 bits de mantisse, 8 d‚Äôexposant). Il offre une haute pr√©cision et une large plage dynamique (~10^38), ce qui en fait la r√©f√©rence en entra√Ænement, au prix d‚Äôun co√ªt m√©moire et calcul √©lev√©.

BF16 (bfloat16) : format 16 bits √† 8 bits d‚Äôexposant (m√™me range que FP32) mais 7 bits de mantisse. Il conserve donc la m√™me dynamique que FP32 tout en r√©duisant la pr√©cision. Tr√®s utilis√© en entra√Ænement mixte pr√©cison sur TPU/GPU car il pr√©serve l‚Äô√©chelle des gradients.

FP16 (float16) : format 16 bits IEEE (5 bits d‚Äôexposant, 10 de mantisse). Sa plage de valeurs (~6.5√ó10^4) est plus r√©duite que BF16, mais il offre plus de pr√©cision significative. C‚Äôest le standard en inf√©rence GPU classique, alliant pr√©cision suffisante et rapidit√© (Tensor Cores d√©di√©s sur GPU Ampere/Hopper).

FP8 (float8) : format flottant 8 bits introduit avec NVIDIA Hopper (H100). Deux variantes existent : E4M3 (4 bits d‚Äôexposant, 3 de mantisse) et E5M2 (5 bits d‚Äôexposant, 2 de mantisse). Elles offrent une dynamique beaucoup plus faible que FP16 (e.g. E4M3 repr√©sente des valeurs jusqu‚Äô√† ¬±448 seulement, E5M2 jusqu‚Äô√† ~¬±5.7√ó10^4) docs.nvidia.com . H100 prend en charge FP8 nativement via la Transformer Engine et ses Tensor Cores, ce qui permet de diviser par deux l‚Äôempreinte m√©moire par rapport √† FP16 et d‚Äôaugmenter fortement le d√©bit, tout en maintenant une qualit√© proche de FP16 si le mod√®le est bien calibr√© (les valeurs extr√™mes doivent √™tre trait√©es pour √©viter l‚Äôinstabilit√©). Par exemple, il faut souvent appliquer des recettes de quantization (per-tensor scaling, choix E4M3 vs E5M2 sur certaines couches) pour obtenir une inf√©rence FP8 stable.

INT8 (entier 8 bits) : en quantization INT8 naive, chaque poids ou activation est r√©duit √† un entier cod√© sur 8 bits (0‚Äì255 ou ‚Äì128 √† 127). Sans ajustement, cela entra√Ænerait une perte importante d‚Äôinformation (beaucoup de poids sont de petites d√©cimales proches de 0, qui deviendraient 0 une fois arrondis) medium.com . C‚Äôest pourquoi on utilise des √©chelles (scales) par canal ou par tenseur pour mapper la plage de valeurs r√©elles aux 256 niveaux disponibles. Deux cas d‚Äôusage : (a) poids uniquement (on ne quantifie que les poids en INT8, les activations restant en FP16/BF16), ou (b) W8A8 (poids et activations int8). La m√©thode SmoothQuant a d√©montr√© qu‚Äôon pouvait obtenir un INT8 stable poids+activations en d√©placement la difficult√© de quantization des activations vers les poids par un simple rescaling pr√©alable arxiv.org . Les GPU Ampere/Hopper disposent de Tensor Cores INT8 qui acc√©l√®rent ces calculs entiers. Un mod√®le W8A8 bien quantifi√© peut d√©livrer des performances proches de FP16 en √©tant deux fois plus l√©ger en m√©moire.

INT4 (entier 4 bits) : quantization ultra-agressive o√π chaque poids est repr√©sent√© sur 4 bits (16 niveaux seulement). En pratique, on n‚Äôapplique l‚ÄôINT4 qu‚Äôaux poids (weight-only) car quantifier les activations √† 4 bits est extr√™mement difficile sans r√©entra√Æner le mod√®le. L‚ÄôINT4 est pris√© pour compresser les grands mod√®les et permettre leur ex√©cution sur des hardwares contraints (GPU moyen, CPU, mobile) ou pour multiplier le nombre de sessions sur une VRAM donn√©e. Des techniques r√©centes comme AWQ ou GPTQ parviennent √† quantifier des LLM en 4 bits avec des pertes de qualit√© modestes, en utilisant par exemple des √©chelles par groupe de poids ou en s√©lectionnant quelques poids ¬´ critiques ¬ª √† garder en plus haute pr√©cision arxiv.org arxiv.org . L‚ÄôINT4 n‚Äôest pas directement acc√©l√©r√© par le mat√©riel (on simule du 4-bit en agr√©geant dans des mots 8 bits/16 bits), donc le gain de vitesse n‚Äôest pas aussi √©lev√© que le gain m√©moire, mais l‚Äôempreinte r√©duite (4√ó plus compacte que FP16) est un atout pour d√©ployer des mod√®les localement.

Pourquoi pr√©ciser ¬´ poids + activations ¬ª ? De nombreuses m√©thodes de quantization ne compressent que les poids du mod√®le, car ce sont eux qui d√©terminent la taille du mod√®le sur disque et en m√©moire. Toutefois, m√™me si les poids sont quantifi√©s en INT4/8, le calcul des activations lors de l‚Äôinf√©rence se fait souvent en FP16, ce qui limite le gain de vitesse. Passer en W8A8 (poids et activations en 8 bits) permet de tirer parti de la quantization sur l‚Äôensemble du calcul (GEMM, etc.), d‚Äôo√π l‚Äôint√©r√™t des solutions comme SmoothQuant ou FP8 qui traitent aussi les activations. En revanche, quantifier les activations est d√©licat car leurs distributions varient avec les entr√©es ; d‚Äôo√π l‚Äôimportance de la calibration.

R√©sum√© des formats :

Format Bits (total) Exposant/Mantisse Plage dynamique approx. Utilisation typique et remarques FP32 32 bits 8 exp, 23 mant ~1e-38 √† 1e+38 Haute pr√©cision (r√©f√©rence). Entra√Ænement, calculs sensibles (somme de pertes, etc.). BF16 16 bits 8 exp, 7 mant ~1e-38 √† 1e+38 (m√™me range FP32) Entra√Ænement mixte pr√©cision (TPU/GPU), inf√©rence. M√™me dynamique que FP32 mais pr√©cision r√©duite (mantisse courte). FP16 16 bits 5 exp, 10 mant ~1e-4 √† 6.5e+4 Inf√©rence sur GPU (Tensor Cores). Pr√©cision suffisante dans la plupart des cas, range plus limit√© que BF16. FP8 E4M3 8 bits 4 exp, 3 mant ~1e-2 √† ~4.5e+2 Inf√©rence GPU Hopper (H100). Faible pr√©cision, range mod√©r√©. Utilis√© pour poids/activations forward (pr√©cision n√©cessaire) docs.nvidia.com . Calibration imp√©rative (Transformer Engine). FP8 E5M2 8 bits 5 exp, 2 mant ~1e-2 √† ~5.7e+4 (+‚àû) Utilis√© plut√¥t pour gradients/backward (plus grande dynamique, moins besoin de pr√©cision) docs.nvidia.com . En inf√©rence pure, sert pour KV-cache FP8 (si > E4M3). INT8 (W8A8) 8 bits (entier pur) 256 valeurs (√©chelle configurable) Inf√©rence quantifi√©e poids + activations. Requiert calibration (ex. SmoothQuant) pour √©viter saturation arxiv.org . Support√© sur GPU (Tensor Cores INT8) et CPU (SIMD int8). INT4 (poids) 4 bits (entier pur) 16 valeurs (par poids ou groupe) Compression extr√™me des poids (taille √∑4 vs FP16). L√©g√®re d√©gradation de style/coh√©rence possible si calibration approximative. Utilis√© via AWQ, GPTQ‚Ä¶ Pas de support mat√©riel natif (calcul via int8 simul√©). 3) H100 et FP8 : ce qui change

La g√©n√©ration Hopper (GPU NVIDIA H100) introduit des Tensor Cores prenant en charge directement le FP8, accompagn√©s de la Transformer Engine (TE) qui g√®re automatiquement le passage FP16‚ÜîFP8 selon un ‚Äúrecipe‚Äù optimal. L‚Äôint√©r√™t principal est de doubler le d√©bit et r√©duire de moiti√© la m√©moire par rapport √† du FP16, pour une perte de qualit√© minime si la quantization est bien calibr√©e. Des benchmarks officiels montrent qu‚Äôun H100 ex√©cutant un mod√®le en FP8 d√©passe largement un A100 en FP16 ‚Äì jusqu‚Äô√† √ó4,6 de throughput en plus, et une latence du premier token ~4,4√ó plus faible sur Llama-2 nvidia.github.io .

Comparaison du throughput maximal de TensorRT-LLM sur H100 vs A100. La figure ci-dessus montre le d√©bit (tokens/s) obtenu avec TensorRT-LLM sur un GPU H100 (barres vertes, calcul en FP8) compar√© √† un A100 (barres noires, FP16) pour diff√©rents mod√®les et tailles de contexte. On observe par exemple un gain de √ó4,6 sur GPT-J 6B (contexte 2048 tokens) et des acc√©l√©rations de l‚Äôordre de √ó3‚Äì4 sur Llama 2 7B, confirmant l‚Äôavantage majeur du FP8 sur H100 en termes de d√©bit nvidia.github.io . Ces gains s‚Äôaccompagnent de latences sensiblement r√©duites : en mode haute performance (beaucoup de requ√™tes parall√®les), H100 FP8 maintient ~100 ms de latence pour le 1er token contre ~480 ms sur A100 FP16 nvidia.github.io . En mode basse latence (batch 1), H100 peut descendre sous les 10 ms pour le 1er token gr√¢ce √† FP8. En pratique, sur un serveur 2√óH100, le FP8 devient le sweet spot optimisant √† la fois la qualit√©, la latence et la VRAM utilis√©e.

Le TensorRT-LLM de NVIDIA (voir section 6) exploite pleinement ces nouveaut√©s du H100. Il int√®gre en effet le support FP8 natif, l‚Äôin-flight batching (regroupement de requ√™tes pour maximiser le remplissage GPU) et le paged KV-cache pour g√©rer la m√©moire attention de fa√ßon optimale developer.nvidia.com . R√©sultat : sur H100, un mod√®le ex√©cut√© en FP8 atteint des d√©bits in√©dits, souvent 3‚Äì5√ó sup√©rieurs √† la g√©n√©ration pr√©c√©dente, tout en conservant une qualit√© de g√©n√©ration pratiquement inchang√©e. Par pr√©caution, certains d√©ploient un mod√®le FP8 en production avec un second profil FP16 en parall√®le pour comparer la qualit√©, mais les retours indiquent que les diff√©rences sont n√©gligeables si la calibration FP8 est bien faite (ex : plus de 99% de la performance d‚Äôun mod√®le FP16 est pr√©serv√©e en FP8 dans vLLM d‚Äôapr√®s des √©valuations standard developers.redhat.com ).

Le KV-cache : FP16 vs FP8
Par d√©faut, le KV-cache (les cl√©s/valeurs de l‚Äôattention) est maintenu en FP16 lors de l‚Äôinf√©rence, ce qui assure une fid√©lit√© maximale mais consomme beaucoup de VRAM ‚Äì environ deux fois plus qu‚Äôen FP8. Sur un contexte de 16k tokens, le KV-cache FP16 d‚Äôun LLM 30B peut occuper plusieurs Go de VRAM. Passer le KV-cache en FP8 divise par deux cette empreinte, permettant d‚Äôaugmenter la longueur de contexte et/ou le nombre de sessions servies simultan√©ment pour une m√™me m√©moire. La contrepartie est un tr√®s l√©ger risque de perte de qualit√© (puisque les valeurs d‚Äôattention sont un peu moins pr√©cises), mais en pratique les tests montrent un impact quasi nul avec du FP8 calibr√© sur H100 developers.redhat.com .

Les serveurs de g√©n√©ration modernes offrent souvent l‚Äôoption de choisir le dtype du KV-cache ind√©pendamment de celui des poids. On peut par exemple utiliser un mod√®le en FP16 tout en stockant le KV-cache en FP8 pour √©conomiser de la VRAM, ou inversement garder un KV-cache en FP16 avec un mod√®le quantifi√© pour maximiser la fid√©lit√© des attentions. Ce m√©lange des pr√©cisions est tout √† fait possible et contr√¥l√© par des flags (ex : --kv-cache-dtype fp8 dans vLLM, --use_fp8_kv_cache dans TensorRT-LLM). L‚Äôapproche d√©pend de la marge m√©moire dont on dispose : si la VRAM est le facteur limitant, mettre le KV en FP8 est un quick win pour augmenter le contexte servable. √Ä l‚Äôextr√™me, certains explorent m√™me le KV-cache en 4 bits (int4) pour les tr√®s longs contextes, mais c‚Äôest encore exp√©rimental.

Par ailleurs, des algorithmes comme PagedAttention (voir section 6B) am√©liorent l‚Äôusage du KV-cache ind√©pendamment du dtype, en le d√©coupant en pages plus petites pour √©viter la fragmentation. Cette paged KV-cache permet de r√©allouer finement la m√©moire KV et de la partager entre requ√™tes, ce qui r√©duit drastiquement le g√¢chis (moins de zones inutilis√©es) blog.vllm.ai blog.vllm.ai . En pratique, sur un serveur multi-utilisateurs, combiner KV-cache FP8 et PagedAttention offre le meilleur des deux mondes : un KV-cache compact et g√©r√© sans perte, pour servir plus de contextes longs simultan√©ment.

En r√©sum√© : garder le KV-cache en FP16 assure la fid√©lit√© maximale mais consomme beaucoup de VRAM, tandis que le passer en FP8 lib√®re ~50% de m√©moire KV pour un impact n√©gligeable sur la qualit√© si bien calibr√©. Cette optimisation est particuli√®rement utile au-del√† de 8k‚Äì16k tokens de contexte, ou pour h√©berger de nombreux chats √† la fois.

M√©thodes de quantization cl√©s
Plusieurs m√©thodes ont √©merg√© pour quantifier les LLMs de fa√ßon efficace :

SmoothQuant (INT8 W8A8, post-training) ‚Äì Il s‚Äôagit d‚Äôune m√©thode de post-training quantization (PTQ) introduite en 2022-2023, permettant de quantifier en 8 bits √† la fois les poids et les activations. L‚Äôid√©e centrale est de lisser les outliers d‚Äôactivation en transf√©rant une partie de leur amplitude vers les poids, via un simple rescaling proportionnel arxiv.org . En effet, les auteurs ont constat√© que les poids d‚Äôun LLM sont globalement faciles √† quantifier, alors que certaines activations pr√©sentent des pics (‚Äúoutliers‚Äù) rendant la quantization √† 8 bits difficile. SmoothQuant calcule pour chaque couche un facteur d‚Äô√©chelle qui, appliqu√© aux poids, √©quilibre leur distribution vs celle des activations, de sorte que quantifier le tout en INT8 provoque beaucoup moins d‚Äôerreurs. C‚Äôest une approche enti√®rement sans r√©-entra√Ænement (pas de fine-tuning n√©cessaire), applicable √† n‚Äôimporte quel mod√®le. SmoothQuant a d√©montr√© qu‚Äôon pouvait quantifier en 8-bit un mod√®le jusqu‚Äô√† 530 milliards de param√®tres avec une perte de pr√©cision n√©gligeable arxiv.org . Les gains mesur√©s sont jusqu‚Äô√† ~1.5√ó d‚Äôacc√©l√©ration et 2√ó de r√©duction m√©moire, le tout sans d√©grader la qualit√© (√† 0.3‚Äì0.5 pp pr√®s sur les benchmarks). SmoothQuant a √©t√© int√©gr√© √† de nombreux outils (ex : Intel Neural Compressor, MMRazor) et sert de base aux impl√©mentations INT8 sur H100.

AWQ (Activation-aware Weight Quantization, INT4 poids-seul) ‚Äì Cette m√©thode (Lin et al., MLSys 2024) vise √† quantifier les poids en 4 bits de mani√®re robuste, en se basant sur l‚Äôanalyse des activations. AWQ fait l‚Äôhypoth√®se que seuls ~1% des canaux de poids sont vraiment critiques pour la performance, et que ces canaux peuvent √™tre identifi√©s via leur distribution d‚Äôactivation arxiv.org . Concr√®tement, on ex√©cute quelques donn√©es d‚Äô√©talonnage √† travers le mod√®le pour rep√©rer les salient weights (poids dont l‚Äôactivation absolue est √©lev√©e), puis on prot√®ge ces 1% de poids (en les quantifiant sur 8 bits ou en les laissant en FP16), tandis que tous les autres 99% sont quantifi√©s en 4 bits. De plus, AWQ applique un scaling particulier sur ces canaux importants pour r√©duire encore l‚Äôerreur de quantization arxiv.org . L‚Äôint√©r√™t est qu‚Äôil n‚Äôy a pas de calibration fine par backpropagation, et donc pas de risque de sur-ajustement sur le set de calibration : AWQ g√©n√©ralise bien √† d‚Äôautres domaines (code, math, etc.) arxiv.org . Les r√©sultats montrent que AWQ surpasse les m√©thodes ant√©rieures sur du 4-bit, et a m√™me permis pour la premi√®re fois de quantifier correctement des LLM instruction-tuned et multi-modaux en 4 bits arxiv.org . Cette m√©thode a re√ßu le Best Paper Award √† MLSys 2024. En pratique, AWQ est utilis√© pour g√©n√©rer des poids 4-bit de haute qualit√© (ex : les mod√®les 4-bit publi√©s par AWS, et certains ‚ÄúGGUF Q4_K_M‚Äù en sont inspir√©s).

GPTQ (INT3/INT4 poids-seul) ‚Äì Propos√©e fin 2022 arxiv.org , GPTQ est une m√©thode de quantization one-shot (en une passe) qui utilise des informations de second-ordre (approximation Hessienne) pour minimiser la perte de pr√©cision due √† la quantization des poids. Plut√¥t que de quantifier b√™tement chaque poids ind√©pendamment, GPTQ optimise bloc par bloc en calculant l‚Äôerreur induite et en la compensant sur les poids restants du bloc (d‚Äôo√π le nom GPT Quantization car initialement test√© sur GPT-3). L‚Äôalgorithme parvient √† quantifier des mod√®les GPT/LLM jusqu‚Äô√† 175 Md de param√®tres en 3 ou 4 bits par poids, en quelques heures sur un seul GPU arxiv.org , avec une perte de performance quasi nulle par rapport au mod√®le FP16 original. Par exemple, ils montrent qu‚Äôon peut quantifier GPT-NeoX-20B en 3 bits sans d√©gradation significative, et GPT3 175B en 4 bits en ~4h arxiv.org . GPTQ double le taux de compression par rapport aux m√©thodes one-shot pr√©c√©dentes tout en pr√©servant mieux l‚Äôexactitude arxiv.org . Cela a √©t√© rapidement adopt√© dans la communaut√© open-source : de nombreux LLM quantifi√©s partag√©s sur HuggingFace utilisent GPTQ (fichiers .pt, .safetensors avec gptq), et des projets comme AutoGPTQ, Transformers, ExLlama ont des backends optimis√©s pour ces poids GPTQ 4-bit. GPTQ reste une r√©f√©rence pour obtenir une excellente qualit√© en 3‚Äì4 bits sans se compliquer la vie.

LLM.int8 (bitsandbytes) ‚Äì Avant l‚Äôessor de SmoothQuant et consorts, Tim Dettmers et al. (NeurIPS 2022) ont propos√© GPT3.int8() alias LLM.int8(), une approche astucieuse pour faire de l‚ÄôINT8 sans perte sur des mod√®les comme GPT-3 arxiv.org arxiv.org . Leur observation : les poids d‚Äôun transformeur pr√©sentent des outlier features (quelques dimensions activ√©es fortement) qui posent probl√®me si on applique un seul scale int8 sur tout un tenseur. Leur solution : utiliser une quantization vectorielle (par groupe de neurones) avec un facteur d‚Äô√©chelle par colonne de matrice arxiv.org , pour quantifier 99.9% des op√©rations en int8, et isoler les outliers dans une multiplication s√©par√©e en 16 bits arxiv.org . Concr√®tement, on fait du GEMM 8-bit sur la majeure partie des dimensions, et les 0.1% de dimensions les plus ‚Äúdangereuses‚Äù (outliers) sont trait√©es en FP16 en parall√®le. Au final, 99.9% des op√©rations sont int8, ce qui divise par ~2 la m√©moire d‚Äôinf√©rence sans perte de perf mesurable arxiv.org . LLM.int8 a √©t√© impl√©ment√© dans la biblioth√®que bitsandbytes, tr√®s utilis√©e en 2022-2023 pour charger des mod√®les 8-bit sur GPU peu VRAM. Cependant, cette m√©thode weight-only n‚Äôacc√©l√®re pas vraiment le calcul (elle l‚Äôall√®ge juste en m√©moire), et s‚Äôav√®re moins stable que W8A8 ou FP8 sur H100. En pratique sur H100, on lui pr√©f√®rera SmoothQuant ou FP8 qui exploitent pleinement les Tensor Cores, mais LLM.int8 reste utile sur du hardware ne supportant pas W8A8 (ex : A100 o√π on veut √©viter de calibrer).

Piles logicielles : TRT-LLM vs vLLM vs llama.cpp/GGUF A) TensorRT-LLM (NVIDIA) ‚Äì Il s‚Äôagit d‚Äôun nouveau runtime/compilateur optimis√© par NVIDIA pour l‚Äôinf√©rence LLM. TensorRT-LLM (TRT-LLM) prend un mod√®le HuggingFace et le compile en un moteur binaire ultra-performant sp√©cifique √† votre GPU (similaire √† TensorRT classique mais orient√© LLM). Ses points forts : support FP8 natif sur H100, support de l‚ÄôINT8 (SmoothQuant) et INT4 (AWQ) en compilation, utilisation avanc√©e du mat√©riel (Tensor Cores, chargement asynchrone‚Ä¶), le tout avec in-flight batching int√©gr√© (pour g√©rer efficacement des requ√™tes parall√®les de longueurs vari√©es) et paged KV-cache (gestion optimis√©e de la m√©moire attention, r√©utilisation inter-requ√™tes) developer.nvidia.com . TRT-LLM supporte en outre le multi-GPU (Tensor Parallelism, Pipeline Parallelism) et des fonctionnalit√©s comme le streaming de tokens. En pratique, sur H100, c‚Äôest la pile offrant les meilleures latences et throughput absolus, au prix d‚Äôune moindre flexibilit√© (il faut convertir/compiler le mod√®le). NVIDIA fournit des quick-starts et conteneurs NGC facilitant son d√©ploiement. La d√©marche typique : exporter un checkpoint HF en format TRT-LLM, puis builder l‚Äôengine avec les options souhait√©es (--use_fp8 ou --quantize int8 etc.), enfin lancer le serveur trtllm-serve. Une fois compil√©, le moteur peut √™tre invoqu√© via une API C++/Python haute performance. Exemple d‚Äôusage : Sur un serveur 2√óH100, on peut convertir Llama2 70B HF en engine TensorRT-LLM FP8 en quelques minutes, puis servir des requ√™tes gRPC avec une latence <10 ms tok√©nisation comprise. NVIDIA annonce sur Llama2 70B ~4.6√ó plus de throughput qu‚ÄôA100, et ~8√ó en combinant H100+TRT-LLM vs A100 sans TRT:contentReference[oaicite:36]{index=36}:contentReference[oaicite:37]{index=37}. Autrement dit, TRT-LLM explose les scores sur H100 gr√¢ce √† la compilation sp√©cialis√©e et FP8.
Limites : TRT-LLM est focalis√© NVIDIA GPU ‚Äì il ne tourne que sur GPUs NVIDIA avec CUDA >=11.x. Il ne supporte pas toutes les architectures de mod√®le exotiques d√®s leur sortie (il se synchronise sur les principaux mod√®les open-source, mais il peut y avoir du d√©lai). Par exemple, un Llama2 avec certaines modifications pourrait n√©cessiter une mise √† jour du parser TRT-LLM. De plus, un moteur compil√© est sp√©cifique : un engine H100 ne fonctionnera pas sur A100, et vice versa, et n‚Äôest pas portable hors de TRT (on ne peut pas le recharger dans PyTorch). Il faut donc garder le checkpoint HF original en parall√®le au cas o√π l‚Äôon veuille utiliser une autre solution. Malgr√© cela, TRT-LLM √©tant open-source depuis fin 2023:contentReference[oaicite:38]{index=38}, on voit la communaut√© l‚Äôadapter progressivement et ajouter le support de nouveaux mod√®les (ex : Mistral 7B support√© peu apr√®s sa sortie).

B) vLLM (Open-Source) ‚Äì vLLM est un serveur d‚Äôinf√©rence LLM open-source d√©velopp√© par UC Berkeley, pens√© pour la performance optimale tout en restant flexible (int√©gration Python). Sa particularit√© est l‚Äôalgorithme PagedAttention (voir papier SOSP 2023) qui g√®re le KV-cache de fa√ßon quasi optimale en termes de m√©moire. Concr√®tement, vLLM alloue le KV-cache en pages de taille fixe au lieu d‚Äôun gros tensor contigu, et utilise une table de correspondance pour assembler les pages correspondant √† chaque requ√™te blog.vllm.ai . Ainsi, la m√©moire n‚Äôest presque plus fragment√©e : <4% de waste mesur√©, au lieu de 60‚Äì80% sur HuggingFace Transformers ou FasterTransformer runpod.io . Cela permet de servir beaucoup plus de requ√™tes en parall√®le sans saturer la VRAM, surtout sur des contextes longs. vLLM a montr√© des throughput jusqu‚Äô√† 24√ó sup√©rieurs √† HF Transformers et ~3√ó sup√©rieurs √† TGI blog.vllm.ai dans ses benchmarks, gr√¢ce √† cette gestion m√©moire et √† un scheduler optimis√©. En termes de quantization, vLLM supporte depuis la v0.5 le FP8 (W8A8) sur GPUs r√©cents (H100, mais aussi initialement MI300x c√¥t√© AMD):contentReference[oaicite:42]{index=42}. Il supporte √©galement l‚ÄôINT8 W8A8 (SmoothQuant) et le chargement de poids 4-bit (AWQ, GPTQ) via des formats comme AWQ (.pt) ou GGML/GGUF. La commande vllm serve propose un argument --quantization pour sp√©cifier fp8, int8, etc., ainsi que --kv-cache-dtype pour choisir FP8/FP16 sur le KV. C√¥t√© int√©gration, vLLM fournit une API Python tr√®s simple (similaire √† generate de HuggingFace, mais en serveur multi-clients). On peut donc l‚Äôutiliser facilement dans un pipeline d‚Äôapplication. Autre avantage : vLLM int√®gre naturellement du batching dynamique (il regroupe les requ√™tes re√ßues √† la vol√©e tant que possible) et supporte le streaming de la r√©ponse token par token.

En pratique, vLLM est id√©al si on veut une solution 100% open-source, multi-plateformes, tout en ayant des performances de haut niveau. Par exemple, sur un m√™me H100, vLLM en FP8 aura un throughput l√©g√®rement inf√©rieur √† TRT-LLM FP8 (puisque TRT compile tout en kernels C++ optimis√©s), mais vLLM offrira plus de souplesse (changement de mod√®le √† la vol√©e, support multi-GPU moins rigide, etc.). Sur des contextes tr√®s longs ou des charges multi-users impr√©visibles, PagedAttention peut m√™me donner l‚Äôavantage √† vLLM en efficacit√©. Le choix entre TRT-LLM et vLLM se fait donc entre performance maximale absolue (TRT) et flexibilit√© OSS (vLLM), sachant que vLLM est d√©j√† extr√™mement performant compar√© aux serveurs traditionnels.

C) llama.cpp / GGUF (CPU & autres) ‚Äì llama.cpp d√©signe √† l‚Äôorigine une impl√©mentation C++ minimaliste pour ex√©cuter LLaMA sur CPU. Depuis, l‚Äô√©cosyst√®me s‚Äôest √©tendu pour supporter de nombreux mod√®les et quantizations, avec le format GGUF (successeur de GGML) pour stocker les poids quantifi√©s. Les atouts de llama.cpp : c‚Äôest multiplateforme (CPU, GPU non-CUDA, Apple Silicon‚Ä¶), tr√®s facile √† d√©ployer (un ex√©cutable unique), et il existe une multitude de variants/UI (text-generation-webui, etc.) l‚Äôutilisant. Il prend en charge des quantizations sp√©cialis√©es not√©es par des suffixes (Q4_0, Q4_K_M, Q5_1, Q8_0, etc.). Par exemple, Q8_0 correspond √† une quantization 8-bit non group√©e (poids sur 8 bits, sans calibration particuli√®re) ‚Äì en pratique proche d‚Äôune compression sans perte sur les poids. Q4_K_M est un format 4-bit avec quantization par groupe (K pour groupwise) et pr√©cision Medium (M), offrant un bon compromis entre qualit√© et taille medium.com . Ces formats proviennent des travaux comme GPTQ, AWQ, et de nombreuses exp√©rimentations communautaires. On peut convertir un mod√®le HF en GGUF quantifi√© via des outils (ex : convert-hf-to-gguf.py + quantize fournis dans llama.cpp qwen.readthedocs.io qwen.readthedocs.io ). Une fois en GGUF, le mod√®le peut √™tre ex√©cut√© via llama.cpp ou des variantes comme text-gen-webui, parfois m√™me charg√©s dans des runtimes sp√©cifiques (ex : acc√©l√©ration GPU via exllama pour Q4). Utilisation typique : llama.cpp/GGUF est parfait pour le prototypage local, le d√©ploiement sur des machines sans GPU puissant, ou le partage communautaire de mod√®les quantifi√©s. Par exemple, on peut faire tourner un LLM 30B 4-bit sur un laptop CPU haut de gamme, certes lentement mais sans d√©pendre de CUDA. Sur GPU, llama.cpp utilise plut√¥t la VRAM via CUDA ou Metal (acc√©l√©ration partielle), mais reste moins optimis√© que TRT-LLM ou m√™me que HuggingFace Transformers sur GPU (puisqu‚Äôil n‚Äôutilise pas les Tensor Cores tr√®s efficacement). Donc pour un H100 on favorisera TRT-LLM ou vLLM, mais pour un edge server ou une machine h√©t√©rog√®ne, llama.cpp offre une universalit√© appr√©ciable.

Compatibilit√© : Un point important est que les engines TRT-LLM ou m√™me les mod√®les vLLM ne sont pas interop√©rables avec llama.cpp, et vice-versa. Un mod√®le GGUF doit √™tre reconverti pour √™tre servi en vLLM ou TRT, ce qui n√©cessite de repartir du checkpoint HF initial le plus souvent. Il est donc recommand√© de conserver le checkpoint HuggingFace d‚Äôorigine de chaque mod√®le, et de ne consid√©rer les conversions (engine TensorRT, quant GGUF‚Ä¶) que comme des builds d√©riv√©s pour un usage sp√©cifique.

¬´ 8-bit ¬ª sous vLLM : que choisir ?
Si vous utilisez vLLM et que vous souhaitez r√©duire la pr√©cision pour gagner en vitesse/m√©moire, deux options 8-bit s‚Äôoffrent √† vous : FP8 ou INT8. Sur mat√©riel NVIDIA H100, la recommandation est g√©n√©ralement d‚Äôopter pour FP8 (W8A8), car c‚Äôest ce qui offre le meilleur compromis performance/qualit√©. Par exemple, en FP8, un H100 peut diviser par deux la latence inter-token par rapport √† FP16 developers.redhat.com . Pour activer ce mode dans vLLM :

vllm serve $MODEL_ID
--quantization fp8
--kv-cache-dtype fp8
--max-model-len 16384

Avec ces param√®tres, vLLM quantifiera √† la vol√©e le mod√®le en 8-bit flottant (poids et activations) et stockera le KV-cache en FP8, tout en fixant une longueur de contexte max de 16k. Il utilise pour cela les Tensor Cores FP8 du GPU (ou les unit√©s AI correspondantes si AMD MI300x). La d√©gradation de qualit√© est minime si le mod√®le est de taille raisonnable et a √©t√© calibr√© correctement (la plupart du temps on peut quantifier un LLM 13B/70B en FP8 sans changement notable dans ses g√©n√©rations developers.redhat.com ).

L‚Äôalternative est l‚ÄôINT8 (W8A8), c‚Äôest-√†-dire la quantization 8-bit enti√®re de SmoothQuant. Celle-ci est utile si, pour une raison ou une autre, vous ne souhaitez pas du FP8 (par ex. pas de GPU Hopper). On activerait alors :

vllm serve $MODEL_ID
--quantization int8
--kv-cache-dtype fp8
--max-model-len 16384

Ici on quantifie les poids+acts en INT8. SmoothQuant √©tant int√©gr√©, la robustesse est normalement assur√©e ‚Äì l√† encore, la qualit√© devrait rester tr√®s proche du FP16 d‚Äôorigine sur les tests usuels. √Ä noter : vLLM supporte aussi le chargement de mod√®les d√©j√† quantifi√©s (ex : --quantization awq pour du 4-bit AWQ), mais en pratique on obtiendra de meilleures perfs en quantifiant √† la vol√©e en int8 ou fp8, car cela permet d‚Äôutiliser les Tensor Cores 8-bit.

Une source de confusion peut venir de bitsandbytes : dans HuggingFace Transformers, on utilisait load_in_8bit=True (bitsandbytes LLM.int8) pour charger un mod√®le en 8-bit poids seulement. Ce n‚Äôest pas la m√™me chose que le --quantization int8 de vLLM, qui lui signifie W8A8 (poids et acts 8-bit). Bitsandbytes n‚Äôest pas n√©cessaire avec vLLM, celui-ci g√®re nativement le 8-bit complet. Par ailleurs, bitsandbytes n‚Äôapporte pas d‚Äôacc√©l√©ration : c‚Äô√©tait surtout utile sur les GPU 16/32 Go pour caser des grands mod√®les en RAM. Sur H100, FP8 ou INT8 via TensorRT/vLLM seront nettement plus efficaces.

Recommandations concr√®tes (cas 2√óH100)
Supposons une machine dual-GPU H100 80 Go sur laquelle on veut d√©ployer un ou plusieurs LLM de ~70 milliards de param√®tres avec contexte long. Objectif : maximiser le throughput et la densit√© de sessions tout en minimisant les r√©gressions de style/coh√©rence du mod√®le (on veut que √ßa reste presque aussi bon qu‚Äôen FP16). Voici quelques recommandations pratiques :

FP8 de bout en bout ‚Äì Si votre mod√®le cible supporte bien FP8, c‚Äôest l‚Äôoption √† privil√©gier sur H100. C‚Äôest-√†-dire poids et activations en FP8, et KV-cache en FP8. La qualit√© sera ~√©quivalente √† FP16 d‚Äôapr√®s les √©valuations (99%+ conserv√©) developers.redhat.com , tandis que la vitesse et l‚Äôempreinte m√©moire seront bien meilleures. En TensorRT-LLM, activer --use_fp8 --use_fp8_kv_cache permet cela (sous r√©serve d‚Äôavoir un GPU SM89). En vLLM, utiliser --quantization fp8 --kv-cache-dtype fp8. Cette config d√©livre g√©n√©ralement le top en qualit√©/perf sur H100.

INT8 SmoothQuant (W8A8) ‚Äì Si, pour des raisons de standardisation ou de prudence, vous pr√©f√©rez rester en ¬´ entiers 8-bit ¬ª, alors une quantization SmoothQuant 8-bit est id√©ale. Elle est tr√®s stable (peu ou pas de perte sur des mod√®les bien connus arxiv.org ) et b√©n√©ficie aussi de l‚Äôacc√©l√©ration Tensor Core int8. Par rapport √† FP8, l‚Äôinconv√©nient potentiel est une l√©g√®re perte de perf (INT8 vs FP8, sur H100 le FP8 est un peu plus rapide) et la n√©cessit√© d‚Äôun petit calibrage des √©chelles SmoothQuant (quoique c‚Äôest g√©n√©ralement fourni ou trivial √† faire). En bref, INT8 W8A8 est un choix ‚Äús√ªr‚Äù et universel si FP8 pose probl√®me.

INT4 AWQ (+ KV FP8) ‚Äì Pour maximiser le nombre de mod√®les/sessions dans la VRAM, on peut descendre √† 4 bits sur les poids. Une approche √©prouv√©e est AWQ 4-bit sur les poids, combin√©e √† un KV-cache en FP8. On obtient ainsi un mod√®le extr√™mement compact (taille divis√©e par 4 vs FP16, donc un 70B tient dans ~40 Go) tout en conservant les activations en 16 bits pour le calcul. La qualit√© en prend un l√©ger coup (quelques points de perplexit√© en plus, style parfois un peu moins fin), mais pour beaucoup d‚Äôusages √ßa reste acceptable ‚Äì on parle de l√©g√®re r√©gression de coh√©rence, pas d‚Äôun effondrement. AWQ ayant d√©montr√© une excellente g√©n√©ralisation, le mod√®le 4-bit se comportera correctement sur des entr√©es vari√©es, avec peut-√™tre un peu plus de r√©p√©titions ou de r√©ponses st√©r√©otyp√©es. Si la priorit√© est de pouvoir faire tourner 2 instances de mod√®le sur 2√óH100 (par ex. deux 70B), l‚ÄôINT4 est quasiment le seul moyen. Dans ce cas, il faudra bien tester sur quelques prompts sensibles pour v√©rifier que la d√©gradation de qualit√© reste tol√©rable dans votre cas d‚Äôusage.

R√®gles simples de validation : Quelle que soit la quantization choisie, il est conseill√© de calibrer et tester le mod√®le sur un jeu de prompts repr√©sentatif de l‚Äôusage r√©el. Par exemple, si vos utilisateurs font du dialogue en fran√ßais sur 4‚Äì8 k tokens, pr√©parez ~20‚Äì50 prompts de ce type (questions ouvertes, suivies de r√©ponses attendues) et comparez les outputs du mod√®le FP16 vs quantifi√© (FP8/INT8/INT4) en blind test. Outre le jugement humain, on peut regarder des m√©triques automatiques (perplexit√© sur un corpus, similarit√© d‚Äôembeddings, mesure de diversit√© distinct-n, etc., ainsi que des taux de refus ou d‚Äôhallucination si c‚Äôest critique pour vous). Ces tests permettront de rep√©rer si, par exemple, le mod√®le quantifi√© a plus tendance √† divaguer ou √† r√©p√©ter des phrases. G√©n√©ralement, en ajustant l√©g√®rement les param√®tres de d√©codage on peut compenser : p.ex. augmenter le repetition_penalty (de 1.1 √† 1.2) aide souvent un mod√®le quantifi√© √† √©viter le rambling. Pour des mod√®les multilingues, assurez-vous de tester dans les langues principales de l‚Äôusage (un quant peut avoir un l√©ger biais vers l‚Äôanglais si on ne fait pas gaffe, selon les outliers de certaines tokens). Enfin, pour le contexte long (‚â•16k), pr√©voyez imp√©rativement le KV-cache en FP8 si la VRAM est juste, sinon vous risquez l‚ÄôOOM avant d‚Äôatteindre la limite de tokens.

En r√©sum√©, sur 2√óH100, on pourra pr√©parer deux profils par mod√®le : un profil haute qualit√© (FP8 end-to-end) et un profil haute densit√© (INT4 ou INT8 selon besoin). Ensuite, en fonction de la charge, on utilise l‚Äôun ou l‚Äôautre. Par exemple, heures creuses : on peut privil√©gier FP8 pour qualit√© optimale ; heures pleines : basculer en INT4 pour servir plus de requ√™tes simultan√©ment. L‚Äôimportant est d‚Äôautomatiser ces bascules proprement si on le fait (certains orchestrateurs peuvent allouer dynamiquement une version quantifi√©e du mod√®le selon l‚ÄôURL de requ√™te ou autre).

Pipelines type (d√©ploiement)
Voici diff√©rents pipelines et configurations courantes pour la mise en production de LLM quantifi√©s :

Pipeline A ‚Äî TRT-LLM FP8 (recommand√© sur H100)

Exporter le mod√®le HF au format TensorRT-LLM :

python3 examples/llama/convert_checkpoint.py
--model_dir /models/YourModelHF
--output_dir /out/trtllm_ckpt
--dtype float16 --tp_size 2

(Cette √©tape transforme le checkpoint HuggingFace en un checkpoint TensorRT-LLM en FP16, avec ici un Tensor Parallelism TP=2 pour 2 GPU.)

Builder le moteur TensorRT-LLM en FP8 :

trtllm-build
--checkpoint_dir /out/trtllm_ckpt
--output_dir /out/engine_fp8_tp2
--tp_size 2 --max_batch_size 16
--max_input_len 16384 --max_output_len 1024
--use_fp8 --use_fp8_kv_cache

Ici on compile le moteur avec quantization FP8 (poids+acts) et KV-cache FP8, pour batch jusqu‚Äô√† 16 et contexte 16k. Le builder va optimiser le plan d‚Äôex√©cution en fonction de ces contraintes.

Lancer le serveur :

trtllm-serve --engine_dir /out/engine_fp8_tp2 --port 8000

Cela lance un serveur gRPC/HTTP local √©coutant sur le port 8000, pr√™t √† recevoir des requ√™tes de g√©n√©ration. Le serveur g√®re le streaming, le batching dynamique, etc. (cf. docs NVIDIA).

üëâ R√©f√©rences : la documentation officielle de TRT-LLM (Overview, Quick Start) d√©taille ces √©tapes developer.nvidia.com nvidia.github.io . En g√©n√©ral, ce pipeline FP8 offre la meilleure latence token et d√©bit par GPU sur H100.

Pipeline B ‚Äî TRT-LLM INT8 SmoothQuant

Si on vise du 8-bit strict (pas de FP8), on peut utiliser le builder TRT-LLM en mode INT8. Il faut d‚Äôabord calibrer SmoothQuant (soit utiliser leur script de calibration avec quelques donn√©es, soit charger un mod√®le d√©j√† smoothquant√©). Ensuite :

Export HF ‚Üí TRT-LLM checkpoint en FP16 (idem √©tape 1 ci-dessus).

Calibration SmoothQuant : TRT-LLM fournit une option --quantize int8 lors du build, qui n√©cessite de pointer vers un dataset de calibration (quelques centaines de phrases). Il applique alors SmoothQuant en interne arxiv.org . Alternativement, on peut smoothquantiser le mod√®le hors-ligne (ex : script SmoothQuant.py du repo NVIDIA).

Build INT8 :

trtllm-build ... --quantize int8 --use_fp8_kv_cache ...

(on recommande KV-cache en FP8 m√™me si mod√®le en INT8, pour gagner de la VRAM).

Serve via trtllm-serve comme avant.

Ce pipeline donne un moteur 8-bit poids+acts. La qualit√© sera tr√®s proche du FP16 (SmoothQuant garantit peu de perte), le throughput un peu en-de√ß√† du FP8 (mais tout de m√™me meilleur que FP16). C‚Äôest utile si l‚Äôon veut absolument √©viter FP8 ou si le mod√®le se quantifie mal en FP8 pour une raison quelconque. Les publications originales de SmoothQuant fournissent plus de d√©tails sur la calibration utilis√©e arxiv.org arxiv.org .

Pipeline C ‚Äî vLLM FP8 ou INT8

Cette approche est ultra-simple : pas de build lourd, on utilise vLLM directement avec le mod√®le HuggingFace. Exemple en FP8 :

vllm serve mistralai/Mistral-7B-Instruct
--quantization fp8
--kv-cache-dtype fp8
--max-model-len 8192

Cela va charger le mod√®le en FP16 puis appliquer la quantization FP8 √† la vol√©e (avec support Hopper requis). On pourrait choisir int8 √† la place. L‚Äôargument --max-model-len fixe le contexte max (important pour allouer le KV-cache). Ce pipeline convient si on veut une solution 100% Python/OSS int√©grable facilement. On peut derri√®re appeler vLLM via son endpoint HTTP ou son client Python. Les performances en FP8 sont excellentes ‚Äì le blog vLLM rapporte jusqu‚Äô√† 2√ó de gain en latence et 3√ó en throughput dans certains cas en passant FP16 ‚Üí FP8 developers.redhat.com . Il g√®re aussi nativement le PagedAttention. Donc pour un d√©ploiement custom (ex : dans un script FastAPI), vLLM est tout indiqu√©.

üëâ R√©f√©rences : la documentation de vLLM (readthedocs) et le papier PagedAttention blog.vllm.ai blog.vllm.ai .

Pipeline D ‚Äî llama.cpp / GGUF (Q8_0 / Q4_K_*)

Enfin, pour le prototypage ou l‚Äôembarqu√©, on peut utiliser un export GGUF. Par exemple, on convertit un mod√®le en 4-bit GPTQ ou AWQ puis en .gguf via convert-hf-to-gguf.py. Il existe aussi des repos HuggingFace proposant directement des fichiers quantifi√©s (ex : model-q4_K_M.gguf). Ensuite, on lance le binaire main de llama.cpp ou une UI qui l‚Äôutilise. L‚Äôavantage est la simplicit√© : pas besoin de d√©pendances NVIDIA, on peut d√©ployer sur un petit serveur CPU ou un Jetson. Les formats de quantization GGUF disponibles incluent Q2_K, Q3_K_M, Q4_0, Q4_K_M, Q5_0, Q5_K_M, Q6_K, Q8_0 qwen.readthedocs.io . Par exemple, Q8_0 est quasiment du FP16 compress√© en 8 bits (peu de perte), Q4_K_M est un 4-bit calibr√© ¬´ Medium ¬ª.

Ce pipeline est utile pour partager un mod√®le open-source facilement : on fournit juste le .gguf quantifi√©, et chacun peut le lancer. Il est aussi pris√© pour les d√©mos web o√π le backend peut √™tre un CPU cost-efficient. √âvidemment, sur H100 on ne va pas utiliser llama.cpp (ce serait g√¢cher du potentiel), mais ce pipeline D reste compl√©mentaire pour d‚Äôautres environnements.

FAQ rapides
Q1 ‚Äì Pourquoi tous les mod√®les ne ‚Äútournent‚Äù pas en TensorRT-LLM ? TRT-LLM, bien qu‚Äôefficace, requiert de supporter explicitement l‚Äôarchitecture du mod√®le. S‚Äôil s‚Äôagit d‚Äôun mod√®le Transformer standard (GPT, LLaMA, etc.), c‚Äôest bon. Mais pour des mod√®les avec couches sp√©ciales ou configurations non conventionnelles, il faut que NVIDIA mette √† jour le parser/les kernels. Par exemple, un mod√®le qui introduit un nouveau type d‚Äôattention ou de feed-forward devra peut-√™tre attendre une prise en charge. De plus, TRT-LLM √©tant centr√© NVIDIA, les auteurs de mod√®les open-source ne le consid√®rent pas forc√©ment comme cible principale : ils publient en format HF ou GGUF pour la port√©e la plus large possible (CPU/AMD/NVIDIA). Il est donc normal que tout n‚Äôarrive pas instantan√©ment dans TRT-LLM. Toutefois, √©tant open-source, la communaut√© peut contribuer au support de nouveaux mod√®les sur TRT-LLM developer.nvidia.com .

Q2 ‚Äì Puis-je m√©langer un mod√®le FP8 avec un KV-cache FP16 ? Oui. Presque toutes les piles permettent de choisir ind√©pendamment la pr√©cision des poids/activations et celle du KV-cache. Par exemple, dans vLLM on a --quantization fp8 (pour les poids/acts) et --kv-cache-dtype fp16 si on voulait conserver le KV en haute pr√©cision. Inversement, on peut faire un mod√®le FP16 avec KV en FP8. Ce mix precision peut √™tre utile pour peaufiner la qualit√© ou √©conomiser de la VRAM. Dans nos tests, un mod√®le FP8 avec KV en FP16 donne un tr√®s l√©ger mieux sur des t√¢ches tr√®s sensibles (ex : des puzzles logiques complexes), mais cela double la m√©moire KV. √Ä l‚Äôinverse, un mod√®le FP16 avec KV en FP8 est presque indiscernable d‚Äôun full FP16 sur la plupart des outputs, tout en lib√©rant pas mal de VRAM. √Ä vous de voir en fonction de vos contraintes, mais sachez que c‚Äôest possible (TRT-LLM: flags --use_fp8 vs --use_fp8_kv_cache s√©par√©s, vLLM idem, etc.).

Q3 ‚Äì ‚Äú8-bit‚Äù = FP8 ou INT8 ? Le terme 8-bit peut pr√™ter √† confusion car il y a deux familles bien distinctes : le FP8 (float 8-bit) et l‚ÄôINT8 (entier 8-bit). FP8 est une repr√©sentation en virgule flottante sur 8 bits, introduite sur H100 (et partiellement disponible sur certaines acc√©l√©rateurs AMD). INT8 est l‚Äôapproche classique par entiers, support√©e depuis longtemps dans les biblioth√®ques quantization. Les deux visent le m√™me but (r√©duire la pr√©cision √† 8 bits), mais fonctionnent diff√©remment : FP8 a une mantisse/exposant, ce qui lui donne une port√©e plus flexible (valeurs tr√®s petites ou tr√®s grandes) pour une m√™me taille docs.nvidia.com , tandis que INT8 a une dynamique fixe mais aucune ‚Äúmagie‚Äù d‚Äôexposant (il faut bien choisir les √©chelles). W8A8 (weights and activations 8-bit) peut d√©signer l‚Äôun ou l‚Äôautre. Sur H100, 8-bit aura tendance √† signifier FP8 car c‚Äôest ce qui donne le meilleur r√©sultat. Sur A100 ou d‚Äôautres, 8-bit impliquera plut√¥t INT8 (SmoothQuant ou autre). Il est donc toujours bon de pr√©ciser.

Q4 ‚Äì PagedAttention, c‚Äôest quoi d√©j√† ? C‚Äôest la technologie de vLLM qui g√®re la m√©moire du KV-cache en ‚Äúpages‚Äù plut√¥t qu‚Äôen blocs contigus monolithiques. En divisant le KV-cache de chaque requ√™te en petits segments, on peut les allouer et les lib√©rer de mani√®re flexible, un peu comme la m√©moire virtuelle d‚Äôun OS blog.vllm.ai . Ainsi, on √©limine la fragmentation interne/externe (chaque page non utilis√©e peut servir ailleurs) et on permet de partager des pages entre requ√™tes (notamment pour le prefix-batching ou le beam search o√π plusieurs g√©n√©rations partagent le m√™me contexte initial) blog.vllm.ai . L‚Äôeffet est un gaspillage m√©moire quasi nul (<4%) et la possibilit√© de batcher √©norm√©ment de requ√™tes sans exploser la VRAM. PagedAttention n‚Äôa pas d‚Äôimpact sur la qualit√© du mod√®le (c‚Äôest transparent c√¥t√© r√©sultats), mais booste le throughput en permettant une meilleure utilisation du GPU blog.vllm.ai ar5iv.labs.arxiv.org . C‚Äôest vraiment une avanc√©e cl√© pour servir les LLM √† grande √©chelle.

Q5 ‚Äì Et les quants GGUF (Q8_0, Q4_K_M, ‚Ä¶) dont on voit les noms‚ÄØ? Ce sont les diff√©rents presets de quantization utilis√©s avec llama.cpp et d‚Äôautres outils CPU. En gros, Q8_0 signifie 8 bits non group√© (toutes les matrices quantifi√©es globalement, sans offset par groupe), c‚Äôest la version la plus fid√®le (presque sans perte, on gagne surtout en taille m√©moire). Q4_K_M signifie 4 bits, quantization group√©e par blocs de 128 (K), niveau Medium (M) de pr√©cision : en pratique √ßa utilise des √©chelles s√©par√©es par groupes de neurones, ce qui am√©liore la fid√©lit√© par rapport √† un simple 4-bit homog√®ne. Il existe aussi Q4_0 (4-bit de base), Q4_K_S (4-bit grouped Small), Q5_0, Q5_K_M, etc. La qualit√© varie un peu en cons√©quence : Q8_0 est tr√®s proche du mod√®le original, Q4_K_M est l‚Äôun des meilleurs compromis en 4-bit, Q4_0 est plus hasardeux (surtout sur mod√®les >30B). Pour un GPU H100, ces formats ne tirent pas parti du hardware sp√©cial (ils seront trait√©s comme des INT8 en gros), donc on leur pr√©f√©rera FP8/INT8 via TRT-LLM ou vLLM. En revanche, pour un CPU ou un petit GPU, les quants GGUF sont super : ils permettent de tester rapidement un mod√®le sans mobiliser 80 Go de RAM. On peut par exemple lancer un Llama2 13B Q4_K_M sur un PC 16 Go RAM ‚Äì la g√©n√©ration sera lente, mais √ßa fonctionne. Donc ces quantizations ont leur place dans l‚Äô√©cosyst√®me, mais ce ne sont pas celles qu‚Äôon utilisera pour une prod optimale sur H100.

Cas particulier : mod√®les ‚Äúmerge‚Äù et licences
Il existe des mod√®les obtenus par merge (fusion de plusieurs checkpoints) qui posent des questions de licence. Par exemple Luminum-123B est un mod√®le 123 milliards r√©sultant du merge de : Mistral-Large-Instruct-2407 (base), Lumimaid-v0.2-123B, et Magnum-v2-123B huggingface.co huggingface.co . Chacune de ces composantes a sa propre licence :

Lumimaid-123B (aussi appel√© NeverSleep/Lumimaid-v0.2-123B) est en licence CC-BY-NC-4.0 (Creative Commons Attribution Non-Commercial) huggingface.co . Cela signifie usage non commercial uniquement, partage autoris√© tant qu‚Äôon cr√©dite l‚Äôauteur, pas de d√©riv√©s commerciaux.

Mistral-Large-Instruct-2407 est sous licence Mistral AI Research License (MRL) huggingface.co . C‚Äôest une licence propri√©taire de Mistral AI qui autorise l‚Äôusage recherche et le self-hosting non commercial, mais interdit l‚Äôusage commercial sans accord explicite. Elle interdit √©galement de distribuer les poids d√©riv√©s √† des tiers sans passer par un accord avec Mistral AI huggingface.co huggingface.co . En gros, c‚Äôest non-commercial avec des restrictions suppl√©mentaires (pas d‚Äôexploitation commerciale du mod√®le ni de ses d√©riv√©s sans licence payante).

Magnum 123B quant √† lui (si on reprend l‚Äôexemple) a probablement une licence du m√™me acabit (souvent les mod√®les ¬´ roleplay ¬ª sont en Llama2-Community ou autre, on va supposer non-commercial aussi).

En combinant ces mod√®les, Luminum h√©rite des restrictions les plus fortes de chacun. Autrement dit, Luminum-123B est non-commercial (√† cause de Lumimaid CC-BY-NC et Mistral MRL) et ne peut pas √™tre distribu√© librement en tant que poids merge sans accord (surtout √† cause de Mistral MRL qui impose de ne pas partager de d√©riv√©s). Pour cette raison, l‚Äôauteur de Luminum a publi√© son mod√®le sur HuggingFace mais en marquant qu‚Äôil faut accepter la MRL pour y acc√©der, et en rappelant qu‚Äôil ne faut pas utiliser √ßa commercialement.

Cons√©quence pratique : si vous quantifiez un mod√®le issu d‚Äôun merge sous restriction non-commerciale, vous ne pouvez pas republier les poids quantifi√©s (m√™me en GGUF ou engine TRT) en pr√©tendant lever la restriction ‚Äì la quantization ne change pas la licence du contenu. Il faut traiter cela comme un mod√®le original pour la licence. Donc, pas de distribution publique de Luminum quantifi√© sans autorisation. √Ä la place, ce qu‚Äôon peut faire c‚Äôest partager des instructions de reproduction (par ex. un script de merge + quantization que chacun peut ex√©cuter de son c√¥t√© apr√®s avoir accept√© les licences sources). On peut aussi √©ventuellement distribuer des delta weights ou LoRA si la licence le permet (par ex. Lumimaid √©tant open non-commercial, un LoRA dessus reste NC).

En somme, faites bien attention aux licences des mod√®les et de leurs donn√©es d‚Äôentra√Ænement. Un mod√®le comme Llama2 70B base est Llama2-community (autorisation commerciale), mais sa version fine-tun√©e par X peut √™tre Apache-2.0 ou NC, etc. Toujours v√©rifier sur la carte HuggingFace ! Dans le doute, abstenez-vous de diffuser un d√©riv√©.

(Exemple r√©el : Luminum √©tant NC, un utilisateur ne doit pas l‚Äôutiliser dans un produit payant. S‚Äôil voulait une version commerciale, il devrait entra√Æner ou acqu√©rir un mod√®le √©quivalent sous licence permissive. Mistral AI vend une licence pro pour son 7B instruct, par exemple.)

Choisir sa quantization (arbre de d√©cision)
Pour cl√¥turer, voici un petit guide d√©cisionnel pour choisir le bon niveau de quantization selon vos besoins :

Qualit√© quasi FP16 + perfs maximales (GPU H100) : Optez pour le FP8 (W8A8). C‚Äôest id√©al si vous avez des H100 ou MI300 r√©cents : vous obtiendrez le meilleur d√©bit et des r√©ponses presque identiques √† FP16. Stacks conseill√©es : TensorRT-LLM si vous visez les toutes meilleures latences et un d√©ploiement C++ optimis√© developer.nvidia.com , ou vLLM en FP8 si vous voulez rester en full Python OSS developers.redhat.com . Dans les deux cas, activez le KV-cache en FP8 pour b√©n√©ficier de la m√©moire gagn√©e.

8-bit ‚Äúclassique‚Äù toutes plateformes : INT8 SmoothQuant (W8A8). Si vos GPU ne supportent pas FP8 (ex : A100) ou si vous tenez √† une solution √©prouv√©e, le combo poids+act en INT8 calibr√© est un excellent choix. SmoothQuant a fait ses preuves sur LLM >100B sans perte significative arxiv.org . Stacks conseill√©es : vLLM --quantization int8, ou des runtimes comme FasterTransformer sur A100 (int8 sans FP8). N‚Äôoubliez pas que INT8 fonctionne aussi bien sur CPU (on commence √† voir des acc√©l√©rations int8 sur CPU via ONNXRuntime par ex).

Compression agressive / VRAM limit√©e : INT4 (AWQ/GPTQ). Si vous devez faire tenir un mod√®le tr√®s gros dans peu de m√©moire, ou lancer plein d‚Äôinstances parall√®les, le 4-bit weight-only est la solution. Vous sacrifierez un peu de ‚Äúhumanit√©‚Äù dans les r√©ponses (phrases un peu plus g√©n√©riques, style moins raffin√©), mais le mod√®le restera fonctionnel pour de nombreuses t√¢ches. Stack conseill√©e : llama.cpp GGUF Q4_K_M ou AutoGPTQ (pour avoir un mod√®le 4-bit utilisable dans Transformers sur GPU). Sur H100, vous pouvez aussi combiner un mod√®le 4-bit avec un KV-cache FP8 via TensorRT-LLM (ils ont montr√© Falcon-180B en INT4 AWQ tournant sur un seul H200 dans un de leurs blogs!).

Prototypage rapide / Edge : GGUF (Q8_0, Q4_K, etc.) via llama.cpp. Si votre but est de tester un mod√®le en local, ou de le d√©ployer sur une machine sans GPU NVIDIA, partez sur les quantizations fournies par la communaut√© en GGUF. √áa √©vite tout tracas d‚Äôinstallation et √ßa marche out of the box. La qualit√© d√©pend du preset (prendre de pr√©f√©rence les versions ‚ÄúK_M‚Äù en 4/5 bits pour un bon √©quilibre qualit√©). N‚Äôesp√©rez pas la m√™me vitesse qu‚Äôavec un GPU pro, mais pour des d√©mos ou du dev c‚Äôest suffisant.

(En cas de doute, commencez par du FP16 ou FP8, voyez si la latence/m√©moire vous conviennent, puis descendez d‚Äôun cran si n√©cessaire. Mieux vaut une r√©ponse un peu lente mais fiable, qu‚Äôun mod√®le compress√© √† outrance mais d√©cevant.)

Commandes types (r√©f√©rence rapide)
Voici un r√©capitulatif de quelques commandes √©voqu√©es, pour r√©f√©rence :

A) TensorRT-LLM (H100, FP8) ‚Äì Exporter un mod√®le HF et builder en FP8 :

Export HF -> TRT-LLM checkpoint
python examples/llama/convert_checkpoint.py
--model_dir /chemin/vers/modele_hf
--output_dir /chemin/vers/output_trtllm_ckpt
--dtype float16 --tp_size 2 # si multi-GPU

Build engine FP8 + KV FP8
trtllm-build
--checkpoint_dir /chemin/vers/output_trtllm_ckpt
--output_dir /chemin/vers/engine_fp8
--use_fp8 --use_fp8_kv_cache
--max_batch_size 8
--max_input_len 8192 --max_output_len 1024
--tp_size 2 # si multi-GPU

Serveur TRT-LLM
trtllm-serve --engine_dir /chemin/vers/engine_fp8 --port 8080

(Cf. docs TRT-LLM pour plus de d√©tails developer.nvidia.com . Pensez √† ajuster batch_size et lengths √† vos besoins r√©els pour optimiser la compilation.)

B) vLLM FP8 (W8A8 + KV FP8) ‚Äì Lancer un serveur vLLM quantifi√© 8-bit :

vllm serve ORGANISATION/MODELE-HF
--quantization fp8
--kv-cache-dtype fp8
--max-model-len 16384

(N√©cessite GPU H100 ou mat√©riel supportant FP8. Cf. vLLM docs developers.redhat.com .)

C) vLLM INT8 (W8A8 + KV FP8) ‚Äì Lancer vLLM en SmoothQuant 8-bit :

vllm serve ORGANISATION/MODELE-HF
--quantization int8
--kv-cache-dtype fp8
--max-model-len 16384

(Fonctionne sur A100/H100. Si pas de FP8 du tout, mettre kv-cache-dtype √† fp16. On peut aussi charger un mod√®le AWQ en passant --quantization awq et en pointant vers le fichier .pt quantifi√©.)

D) Conversion GGUF (llama.cpp) ‚Äì Convertir et quantifier un mod√®le en GGUF :

1. Convertir un mod√®le HF en GGUF FP16
python convert-hf-to-gguf.py NomDuModeleHF --outfile modele.gguf

2. Quantifier en 4 bits par ex.
./quantize modele.gguf modele-q4_0.gguf q4_0

(Voir documentation Qwen/llama.cpp qwen.readthedocs.io qwen.readthedocs.io . Il existe aussi des scripts pour appliquer AWQ avant conversion afin d‚Äôam√©liorer la qualit√© comme vu plus haut.)

Points de contr√¥le (qualit√©)
Avant de d√©ployer en production, pensez √† passer votre mod√®le quantifi√© par quelques points de contr√¥le qualit√© :

Jeu de validation : Pr√©parez un set de prompts vari√©s (10-50, selon vos ressources), couvrant les cas d‚Äôusage typiques. Id√©alement multi-langues si concern√©. Incluez des conversations multi-tours, des questions pi√®ges, des demandes de g√©n√©ration cr√©ative, etc. Faites g√©n√©rer le mod√®le FP16 et le mod√®le quantifi√© sur ces prompts, et comparez. Cherchez les diff√©rences flagrantes (r√©p√©titions, ignorances d‚Äôinstructions, r√©ponses √† c√¥t√©‚Ä¶).

M√©triques auto : Si possible, √©valuez la perplexit√© du mod√®le quantifi√© sur un corpus de test. Un √©cart de perplexit√© tr√®s faible (<5-10%) par rapport au FP16 est bon signe. Vous pouvez aussi calculer des m√©triques de diversit√© lexicale comme distinct-n sur des longues g√©n√©rations : un mod√®le quantifi√© de fa√ßon agressive a parfois tendance √† recycler les m√™mes tournures, ce qui r√©duit distinct-4/5. Enfin, si votre application craint les hallucinations ou les refus injustifi√©s, testez-en quelques-uns (ex : demandes factuelles pour voir si le quant hallucine plus ; requ√™tes sensibles pour voir s‚Äôil se met √† refuser inutilement).

A/B testing : Le mieux reste de faire √©valuer quelques paires de r√©ponses (FP16 vs quant) par des humains sans leur dire qui est qui. S‚Äôils n‚Äôy voient que du feu ou pr√©f√®rent m√™me parfois la version quantifi√©e, c‚Äôest gagn√© üôÇ.

R√©glage des hyperparam√®tres : Un mod√®le quantifi√© peut n√©cessiter de l√©gers ajustements de sampling. En particulier, augmenter le repetition_penalty (p.ex. de 1.1 √† 1.15) peut aider √† garder le style coh√©rent sur de longues r√©ponses. On peut aussi ajuster le top_p ou temperature si on constate des sorties moins vari√©es. N‚Äôh√©sitez pas √† tuner ces param√®tres sur votre set de validation. Parfois, un quant de 4-bit appr√©ciera une temp√©rature un poil plus √©lev√©e pour compenser la perte de finesse.

Long contexte : Si vous visez du 16k ou 32k tokens, testez-le ! Envoyez un prompt de ~15k tokens et voyez si le mod√®le continue correctement. Sur de tr√®s longs contextes, la quantization peut accumuler de l‚Äôerreur num√©rique (d‚Äôo√π l‚Äôint√©r√™t du KV en FP8 ou FP16). Assurez-vous que la d√©gradation reste g√©rable (de toute fa√ßon, au-del√† de 8k m√™me un mod√®le FP16 commence √† flancher parfois).

En suivant ces points de contr√¥le, vous aurez l‚Äôassurance que votre mod√®le quantifi√© tient la route. La quantization est un art subtil : 99% du temps √ßa marche tr√®s bien, mais il vaut mieux d√©busquer le 1% de cas o√π √ßa pourrait poser souci avant que les utilisateurs ne tombent dessus.

TL;DR
H100 = FP8 natif üìà : Les GPU NVIDIA Hopper (H100) supportent nativement le calcul en float8 via la Transformer Engine. Cela permet d‚Äôatteindre des performances jusqu‚Äô√† ~4‚Äì5√ó sup√©rieures √† A100 FP16, avec une qualit√© de mod√®le pratiquement inchang√©e si calibr√© correctement nvidia.github.io developers.redhat.com . En clair, FP8 sur H100 offre le meilleur ratio qualit√©/latence/VRAM aujourd‚Äôhui.

TensorRT-LLM üöÄ : C‚Äôest la solution NVIDIA optimis√©e pour inf√©rence LLM. Elle compile le mod√®le en un engine ultra-rapide. Avantages : support du FP8 et INT8 (SmoothQuant) directement, batching asynchrone en vol, KV-cache pagin√©, multi-GPU‚Ä¶ Bref, c‚Äôest ce qui donnera les latences et throughputs minimum sur H100 developer.nvidia.com . Inconv√©nient : sp√©cifique NVIDIA, et n√©cessite de passer par une √©tape de build.

vLLM üêç : Serveur haute performance open-source. Il introduit PagedAttention qui r√©duit le g√¢chis m√©moire du KV-cache √† <4%, permettant de booster le throughput sans changer de hardware runpod.io . vLLM supporte aussi FP8 et INT8 (ainsi que chargement de mod√®les 4-bit). Id√©al si on veut une int√©gration simple (quelques lignes Python) tout en gardant des perfs state-of-the-art. C‚Äôest open-source (Apache 2.0). Moins rapide que TRT-LLM sur un seul GPU, mais plus flexible.

Choix de quantization ü§ñ :

Pour la qualit√© max : FP8 (8-bit flottant) si possible, sinon INT8 SmoothQuant. Ces deux options donnent des r√©sultats quasi identiques au FP16 original sur la plupart des mod√®les arxiv.org .

Pour pousser la compression : INT4 (4-bit poids) via AWQ/GPTQ est faisable sur des grands mod√®les, au prix d‚Äôune tr√®s l√©g√®re d√©gradation du style/coh√©rence. √Ä utiliser si VRAM limit√©e ou pour h√©berger plusieurs instances.

Le tout sans r√©entra√Æner (PTQ). On peut quantizer un mod√®le apr√®s-coup et le servir directement.

Formats GGUF (llama.cpp) üíæ : Utiles pour ex√©cuter des LLM sur CPU ou petits GPU. Exemples : Q8_0 (8-bit poids), Q4_K_M (4-bit groupe Medium) medium.com . Ils rendent les mod√®les plus accessibles, au prix d‚Äôune vitesse moindre. Sur H100, ces formats ne tirent pas profit du hardware sp√©cialis√©, donc on privil√©giera plut√¥t TRT-LLM/vLLM. Mais pour du offline ou du local sans CUDA, c‚Äôest g√©nial.

Licences & mod√®les merges üìú : Attention √† la l√©galit√© ! Un mod√®le comme Luminum-123B mergeant Mistral (licence MRL, non-commercial) et Lumimaid (CC-BY-NC-4.0) reste Non-Commercial et soumis aux restrictions de diffusion des originaux huggingface.co huggingface.co . Quantizer un mod√®le ne change pas sa licence. Il est g√©n√©ralement interdit de redistribuer des poids d√©riv√©s sans accord si la licence source l‚Äôinterdit (ex : Mistral MRL prohibe de partager le mod√®le fine-tun√© sans passer par eux huggingface.co ). Pr√©f√©rez partager des scripts ou des diffs/LoRA plut√¥t que les poids quantifi√©s directement pour ces cas. En clair : toujours respecter les licences, m√™me pour un mod√®le quantifi√© ou compress√© !

0 commit comments
Comments
0
 (0)
Comment
You're not receiving notifications from this thread.

Update Article.md ¬∑ SoMika00/Quant_llm@c4335f0 There are no files selected for viewing





debut de la reecriture : Guide pratique (2025) ‚Äî Quantization LLM sur H100 et alternatives (TRT-LLM, vLLM, GGUF)
Introduction : Les bases de la m√©moire d'un LLM
Pour bien comprendre la quantization, il est essentiel de ma√Ætriser quatre concepts cl√©s qui d√©terminent l'empreinte m√©moire d'un grand mod√®le de langage (LLM) lors de l'inf√©rence : les poids, les activations, le KV-cache et les formats num√©riques.
Poids et Activations : Un tenseur est un tableau multi-dimensionnel de nombres. Dans un LLM, les poids (les param√®tres du mod√®le appris durant l'entra√Ænement) sont stock√©s sur disque puis charg√©s en m√©moire vive (VRAM du GPU) sous forme de tenseurs. Les activations d√©signent les r√©sultats interm√©diaires calcul√©s √† chaque √©tape de l'inf√©rence.
KV-Cache : La m√©moire KV-cache correspond aux tenseurs de cl√©s (Keys) et valeurs (Values) du m√©canisme d'attention, conserv√©s au fil de la g√©n√©ration de texte. Ce cache est crucial car il acc√©l√®re l'auto-r√©gression en √©vitant de recalculer tout l'historique √† chaque nouveau token g√©n√©r√©. Sa gestion est optimis√©e sur les serveurs modernes, notamment gr√¢ce √† des techniques comme Paged Attention (qui partitionne le cache en pages) et l'in-flight batching (qui regroupe des requ√™tes en cours). La biblioth√®que open-source vLLM, par exemple, utilise PagedAttention pour stocker les Keys/Values en blocs non contigus, r√©duisant le gaspillage de m√©moire √† moins de 4 % (contre 60 √† 80 % dans les syst√®mes classiques).
R√©partition de la VRAM : En pratique, la VRAM requise √† l‚Äôinf√©rence se compose des poids du mod√®le, des activations temporaires, et du KV-cache. Les poids dominent souvent l'empreinte m√©moire (environ 65 % de la VRAM sur un mod√®le 13B), le KV-cache occupant ~30 % (variable selon la longueur de s√©quence) et les activations une part minime. La taille sur disque est donc dict√©e par les poids, tandis que l'usage en VRAM d√©pend fortement du KV-cache pour les longs contextes.
Overflow / Underflow : Un overflow survient quand une valeur d√©passe la plage repr√©sentable par le format num√©rique choisi (elle devient infinie), tandis qu‚Äôun underflow se produit lorsque la valeur est trop petite pour √™tre repr√©sent√©e (elle est arrondie √† z√©ro). Les formats √† faible dynamique, comme FP8 ou INT4/INT8, n√©cessitent des techniques de calibration pour √©viter ces probl√®mes. La m√©thode SmoothQuant, par exemple, ajuste l'√©chelle des poids et des activations pour "lisser" les valeurs extr√™mes (outliers) avant la quantization.
Les formats num√©riques : de la haute pr√©cision √† l'ultra-compression
Le choix du format num√©rique est un compromis permanent entre pr√©cision, performance et consommation m√©moire.
FP32 (float 32 bits) : Le format de r√©f√©rence en entra√Ænement. Il offre une haute pr√©cision et une large plage dynamique (~10^38), au prix d'un co√ªt m√©moire et calcul √©lev√©.
BF16 (bfloat16) : Format 16 bits avec la m√™me plage dynamique que le FP32 mais une pr√©cision r√©duite. Tr√®s utilis√© en entra√Ænement en pr√©cision mixte sur TPU/GPU car il pr√©serve bien l'√©chelle des gradients.
FP16 (float16) : Format 16 bits standard en inf√©rence GPU. Sa plage de valeurs est plus r√©duite que le BF16, mais il offre une meilleure pr√©cision. Les Tensor Cores des GPU Ampere/Hopper y sont d√©di√©s.
FP8 (float8) : Introduit avec l'architecture NVIDIA Hopper (H100), il existe en deux variantes : E4M3 (plus de pr√©cision, plage mod√©r√©e) et E5M2 (moins de pr√©cision, plage plus large). Le H100 l'acc√©l√®re nativement via sa Transformer Engine, permettant de diviser par deux l'empreinte m√©moire par rapport au FP16 avec une qualit√© quasi-identique si le mod√®le est bien calibr√©.
INT8 (entier 8 bits) : La quantization en entiers 8 bits (valeurs de -128 √† 127) n√©cessite des √©chelles (scales) pour mapper la plage de valeurs r√©elles aux 256 niveaux disponibles. On distingue la quantization des poids seuls de la quantization W8A8 (poids et activations), qui permet d'acc√©l√©rer l'ensemble du calcul gr√¢ce aux Tensor Cores INT8. La m√©thode SmoothQuant a d√©montr√© l'efficacit√© du W8A8 en d√©pla√ßant la difficult√© de quantization des activations vers les poids.
INT4 (entier 4 bits) : Une quantization ultra-agressive (16 niveaux) appliqu√©e uniquement aux poids (weight-only). Elle est utilis√©e pour compresser massivement les mod√®les sur du mat√©riel contraint (GPU grand public, CPU). Des techniques comme AWQ ou GPTQ parviennent √† r√©duire la perte de qualit√© en utilisant des √©chelles par groupe de poids ou en prot√©geant les poids les plus importants. Le gain de vitesse n'est pas aussi √©lev√© que le gain m√©moire, car l'INT4 n'est pas acc√©l√©r√© nativement.
Tableau r√©capitulatif des formats
Format	Bits (total)	Exposant/Mantisse	Plage dynamique approx.	Utilisation typique et remarques
FP32	32 bits	8 exp, 23 mant	~1e-38 √† 1e+38	Haute pr√©cision (r√©f√©rence). Entra√Ænement, calculs sensibles.
BF16	16 bits	8 exp, 7 mant	~1e-38 √† 1e+38	Entra√Ænement mixte pr√©cision (TPU/GPU), inf√©rence. M√™me dynamique que FP32 mais pr√©cision r√©duite.
FP16	16 bits	5 exp, 10 mant	~1e-4 √† 6.5e+4	Inf√©rence sur GPU (Tensor Cores). Pr√©cision suffisante dans la plupart des cas.
FP8 E4M3	8 bits	4 exp, 3 mant	~1e-2 √† ~4.5e+2	Inf√©rence GPU Hopper (H100). Utilis√© pour poids/activations. Calibration imp√©rative.
FP8 E5M2	8 bits	5 exp, 2 mant	~1e-2 √† ~5.7e+4	Utilis√© pour les gradients (plus grande dynamique) ou le KV-cache FP8.
INT8 (W8A8)	8 bits (entier)	256 valeurs (√©chelle)	Inf√©rence quantifi√©e poids + activations. Requiert calibration (ex. SmoothQuant).	
INT4 (poids)	4 bits (entier)	16 valeurs (par groupe)	Compression extr√™me des poids. L√©g√®re d√©gradation possible. Utilis√© via AWQ, GPTQ.	
Objectifs et strat√©gies de quantization par d√©faut
La quantization n'est pas une fin en soi, mais un moyen d'atteindre des objectifs pr√©cis de d√©ploiement. Voici trois strat√©gies par d√©faut et leurs cas d'usage.
H100 FP8 de bout en bout (Qualit√© maximale, performance extr√™me)
Objectif : Obtenir la latence la plus faible et le d√©bit le plus √©lev√© possible sur du mat√©riel de derni√®re g√©n√©ration (NVIDIA H100), sans sacrifier la qualit√© du mod√®le.
Description : Utilisation du format FP8 pour les poids, les activations et le KV-cache. Cette approche tire pleinement parti de l'architecture Hopper et de sa Transformer Engine. La perte de qualit√© est g√©n√©ralement n√©gligeable (<1 %) par rapport au FP16, tandis que les performances sont multipli√©es.
Quand ne pas l'utiliser : Si vous ne disposez pas de GPU compatibles (Hopper, Ada ou futurs), ou si des tests r√©v√®lent une instabilit√© num√©rique sur des couches tr√®s sp√©cifiques de votre mod√®le (un cas rare).
INT8 SmoothQuant (Le choix robuste et universel)
Objectif : R√©duire de moiti√© l'empreinte m√©moire et acc√©l√©rer les calculs sur une large gamme de mat√©riel (GPU Ampere/Hopper, CPU), avec une garantie de stabilit√© et une perte de qualit√© minimale.
Description : Quantization des poids et des activations en entiers 8 bits (W8A8) gr√¢ce √† la m√©thode SmoothQuant, qui calibre les √©chelles pour √©viter les erreurs. C'est une solution √©prouv√©e, bien support√©e par les frameworks.
Quand ne pas l'utiliser : Sur H100, le FP8 est souvent l√©g√®rement plus performant. Si la moindre d√©gradation de la qualit√© est inacceptable pour des t√¢ches de raisonnement tr√®s complexes (bien que la perte soit quasi nulle).
INT4 AWQ + KV-Cache FP8 (Compression maximale, haute densit√©)
Objectif : Maximiser le nombre de mod√®les ou de sessions simultan√©es sur une VRAM donn√©e, ou d√©ployer des mod√®les tr√®s larges sur du mat√©riel contraint.
Description : Les poids du mod√®le sont compress√©s en 4 bits avec une m√©thode avanc√©e comme AWQ (Activation-aware Weight Quantization) pour pr√©server la qualit√©. Les activations restent en FP16 et le KV-cache peut √™tre stock√© en FP8 pour √©conomiser encore plus de m√©moire.
Quand ne pas l'utiliser : Pour des applications o√π la finesse stylistique, la coh√©rence sur de tr√®s longs textes ou le raisonnement complexe sont critiques. La perte de qualit√©, bien que modeste, est plus perceptible qu'en 8 bits.
Tableau d√©cisionnel : Quand choisir quoi ?
Ce tableau vous aide √† choisir la bonne strat√©gie en fonction de vos contraintes.
Contrainte	FP8 (W8A8)	INT8 SmoothQuant (W8A8)	INT4 AWQ (poids)	GGUF (CPU/Edge)
Latence minimale	‚úÖ (H100)	‚úÖ (GPU r√©cents)	‚ö†Ô∏è (Gain m√©moire > vitesse)	‚ùå (Lent)
VRAM maximale	‚úÖ	‚úÖ	‚úÖ‚úÖ (Optimal)	‚úÖ (Tr√®s faible conso)
Qualit√© max (proche FP16)	‚úÖ‚úÖ	‚úÖ	‚ö†Ô∏è (L√©g√®re d√©gradation)	‚ö†Ô∏è (D√©pend du format)
Portabilit√© (CPU/GPU)	‚ùå (H100+)	‚ö†Ô∏è (GPU/CPU avec support)	‚ö†Ô∏è (GPU/CPU avec support)	‚úÖ‚úÖ (Universel)
Flag vLLM	--quantization fp8	--quantization int8	--quantization awq	(Chargement direct du .gguf)
Option TensorRT-LLM	--use_fp8	--quant_mode int8_sq	--quant_mode int4_awq	(Non applicable)
La gestion du KV-cache : un enjeu majeur
L'impact du format : FP16 vs FP8
Par d√©faut, le KV-cache est souvent maintenu en FP16 pour une fid√©lit√© maximale, mais cela consomme beaucoup de VRAM. Passer le KV-cache en FP8 divise par deux cette empreinte, permettant d'augmenter la longueur de contexte ou le nombre de sessions simultan√©es. En pratique, l'impact sur la qualit√© est quasi nul avec un FP8 bien calibr√© sur H100.
La formule du KV-Cache (avec GQA)
La taille en octets du KV-cache peut √™tre estim√©e avec la formule suivante :
KV_bytes ‚âà batch_size √ó sequence_length √ó num_layers √ó (2 √ó hidden_size) √ó bytes_per_dtype / gqa_factor
gqa_factor : Ratio entre le nombre de t√™tes d'attention et le nombre de t√™tes de cl√©s/valeurs (pour Grouped-Query Attention). Vaut 1 pour MHA, >1 pour GQA/MQA.
bytes_per_dtype :
Data Type	Bytes
FP32	4
FP16 / BF16	2
FP8 / INT8	1
Conclusion : Passer le KV-cache en FP8 permet de servir environ deux fois plus de sessions ou de g√©rer un contexte deux fois plus long √† VRAM constante.
L'optimisation structurelle : PagedAttention
Ind√©pendamment du format, des algorithmes comme PagedAttention (popularis√© par vLLM) optimisent l'usage du KV-cache en le d√©coupant en pages. Cela √©vite la fragmentation m√©moire et permet de partager des pages entre requ√™tes, r√©duisant drastiquement le gaspillage. Combiner KV-cache FP8 et PagedAttention offre le meilleur des deux mondes : un cache compact et g√©r√© efficacement.
M√©thodes de quantization cl√©s
SmoothQuant (INT8 W8A8) : Lisse les outliers d'activation en transf√©rant une partie de leur amplitude vers les poids via un simple rescaling. C'est une m√©thode de post-training (sans r√©-entra√Ænement) qui permet de quantifier des mod√®les jusqu'√† 530 milliards de param√®tres en 8 bits avec une perte de pr√©cision n√©gligeable.
AWQ (Activation-aware Weight Quantization, INT4) : Identifie et prot√®ge les ~1% de poids les plus critiques pour la performance (identifi√©s via l'analyse des activations) et quantifie agressivement les 99% restants en 4 bits. Cette m√©thode a re√ßu le prix du meilleur article √† MLSys 2024.
GPTQ (INT3/INT4) : Utilise des informations de second ordre (approximation Hessienne) pour minimiser l'erreur de quantization bloc par bloc. C'est une m√©thode one-shot rapide et efficace qui a √©t√© largement adopt√©e par la communaut√© open-source.
LLM.int8() (bitsandbytes) : Une approche qui quantifie la majorit√© des op√©rations en INT8 tout en isolant les outliers dans une multiplication s√©par√©e en FP16. Elle r√©duit l'empreinte m√©moire sans acc√©l√©rer le calcul et est aujourd'hui souvent remplac√©e par des solutions W8A8 plus performantes sur H100.
Piles logicielles : TRT-LLM vs vLLM vs llama.cpp/GGUF
A) TensorRT-LLM (NVIDIA)
Un runtime et compilateur optimis√© par NVIDIA. Il prend un mod√®le et le compile en un moteur binaire ultra-performant, sp√©cifique au GPU.
Points forts : Support FP8 natif sur H100, INT8/INT4, in-flight batching, paged KV-cache, et multi-GPU. Offre les meilleures performances brutes (latence et d√©bit).
Limites : Sp√©cifique √† NVIDIA, moins flexible (la compilation est n√©cessaire), et peut avoir un d√©lai de support pour les nouveaux mod√®les exotiques.
B) vLLM (Open-Source)
Un serveur d'inf√©rence LLM d√©velopp√© par UC Berkeley, ax√© sur la performance et la flexibilit√©.
Points forts : Int√®gre l'algorithme PagedAttention pour une gestion m√©moire quasi-optimale. Supporte FP8, INT8, et le chargement de poids 4-bit (AWQ, GPTQ). API Python simple, batching dynamique et open-source (Apache 2.0).
Limites : L√©g√®rement moins performant que TRT-LLM en termes de latence brute sur une seule requ√™te, mais souvent plus efficace en d√©bit sous forte charge multi-utilisateurs gr√¢ce √† PagedAttention.
C) llama.cpp / GGUF (CPU & autres)
Un √©cosyst√®me C++ pour ex√©cuter des LLM sur du mat√©riel vari√© (CPU, Apple Silicon). GGUF est le format de fichier utilis√© pour stocker les poids quantifi√©s.
Points forts : Multiplateforme, facile √† d√©ployer, large communaut√©. Supporte de nombreux formats de quantization sp√©cialis√©s (Q4_K_M, Q8_0, etc.). Id√©al pour le prototypage local et le d√©ploiement sur des machines sans GPU puissant.
Limites : Non optimis√© pour les Tensor Cores des GPU haut de gamme. Utiliser llama.cpp sur un H100 serait un √©norme gaspillage de potentiel.
Calibration et garde-fous pour FP8/INT8 : une checklist pratique
Une quantization r√©ussie n'est pas automatique. Voici une checklist pour assurer la robustesse.
Jeu de calibration : Utilisez un jeu de donn√©es repr√©sentatif de votre cas d'usage en production (512 √† 2048 prompts suffisent g√©n√©ralement). La qualit√© des donn√©es de calibration influe directement sur la performance du mod√®le quantifi√©.
Exclusion de couches sensibles : Si vous observez une instabilit√©, excluez les couches d'embedding et la t√™te de pr√©diction du langage (lm_head) de la quantization. Ces couches sont souvent plus sensibles √† la perte de pr√©cision.
Recette de quantization :
Scaling : Privil√©giez un scaling per-tensor ou per-channel pour une granularit√© fine.
FP8 : Appliquez une recette mixte. Utilisez E4M3 pour les couches n√©cessitant de la pr√©cision (comme les matrices de poids) et E5M2 pour celles n√©cessitant une plus grande dynamique (comme les gradients, ou parfois les activations avec de grands outliers).
Contr√¥les de validit√© (Sanity Checks) :
Mesurez le taux de saturation et le nombre d'overflows pendant la calibration. Des valeurs √©lev√©es indiquent un probl√®me.
Calculez la similarit√© cosinus entre les sorties du mod√®le FP16 et du mod√®le quantifi√©. Un score √©lev√© (>0.99) est un bon indicateur.
Strat√©gie de repli (Fallback) : En cas de doute, surtout pour des t√¢ches de raisonnement complexe, conservez le KV-cache en FP16 (--kv-cache-dtype fp16). C'est un excellent compromis qui pr√©serve la fid√©lit√© des attentions tout en b√©n√©ficiant de la quantization des poids/activations.
Observabilit√© en production (SLOs)
D√©ployer un mod√®le quantifi√©, c'est bien. Le surveiller, c'est mieux.
Indicateurs cl√©s √† suivre :
Latence : p99 du temps de g√©n√©ration du premier token.
D√©bit : tokens/seconde.
Utilisation VRAM : VRAM par requ√™te, fragmentation du KV-cache.
Erreurs : Taux d'erreurs Out-Of-Memory (OOM), taux d'abandon des requ√™tes.
D√©rive de la qualit√© : Perplexit√© calcul√©e de mani√®re hebdomadaire sur un jeu de donn√©es de contr√¥le.
Alertes √† configurer :
Latence p99 > seuil d√©fini.
Utilisation VRAM > 95%.
Taux OOM > 0.5% des requ√™tes.
Gaspillage du KV-cache > 8%.
Risques, cas limites et comment les g√©rer
Saturation du FP8 E4M3 : Sur des couches √† tr√®s grande dynamique, le format E4M3 peut saturer. La solution est de forcer localement l'utilisation du format E5M2 pour ces couches sp√©cifiques.
Long contexte et raisonnement complexe : Pour des t√¢ches tr√®s sensibles qui s'√©talent sur des dizaines de milliers de tokens, la l√©g√®re perte de pr√©cision du KV-cache FP8 peut s'accumuler. Testez rigoureusement avec un KV-cache FP16 pour valider si la diff√©rence est significative pour votre cas d'usage.
"Perte de style" en INT4 : La quantization agressive en 4 bits peut parfois rendre le style du mod√®le plus g√©n√©rique. On peut souvent compenser ce ph√©nom√®ne en ajustant l√©g√®rement les param√®tres de g√©n√©ration : augmenter un peu repetition_penalty (ex: de 1.1 √† 1.15) et temperature.
Licences : la quantization ne change rien
La quantization est une transformation technique, elle ne modifie en rien la licence d'un mod√®le.
Mod√®le (Exemple)	Licence	Usage Commercial	Redistribution Poids Quantifi√©s	Strat√©gie de Partage Recommand√©e
Llama 3 70B	Llama 3 Community License	‚úÖ	‚úÖ	Poids directs (GGUF, AWQ, etc.)
Mistral Large	Mistral AI Research License (MRL)	‚ùå	‚ùå	Scripts de quantization/merge
Luminum-123B (Merge)	H√©rite de la plus restrictive (MRL, CC-BY-NC)	‚ùå	‚ùå	Scripts de merge + quantization
Gemma 7B	Gemma Terms of Use	‚úÖ	‚úÖ	Poids directs
R√®gle d'or : Un mod√®le issu d'un merge h√©rite des restrictions de toutes ses composantes. En cas de doute, partagez des scripts de reproduction (merge, fine-tuning, quantization) plut√¥t que les poids d√©riv√©s.
Annexe : Commandes types (Cheat-Sheet)
A) TensorRT-LLM (H100, FP8)
Exporter le checkpoint HF
code
Bash
python examples/llama/convert_checkpoint.py \
  --model_dir /path/to/model_hf \
  --output_dir /path/to/output_trtllm_ckpt \
  --dtype float16 --tp_size 2
Builder l'engine FP8
code
Bash
trtllm-build \
  --checkpoint_dir /path/to/output_trtllm_ckpt \
  --output_dir /path/to/engine_fp8 \
  --use_fp8 --use_fp8_kv_cache \
  --max_batch_size 8 \
  --max_input_len 8192 --max_output_len 1024 \
  --tp_size 2
Lancer le serveur
code
Bash
trtllm-serve --engine_dir /path/to/engine_fp8 --port 8080
B) vLLM (FP8 ou INT8)
Lancer en FP8
code
Bash
vllm serve ORG/MODEL-HF \
  --quantization fp8 \
  --kv-cache-dtype fp8 \
  --max-model-len 16384
Lancer en INT8 (SmoothQuant)
code
Bash
vllm serve ORG/MODEL-HF \
  --quantization int8 \
  --kv-cache-dtype fp8 \
  --max-model-len 16384
C) Conversion GGUF (llama.cpp)
Convertir HF en GGUF FP16
code
Bash
python convert-hf-to-gguf.py ModelNameHF --outfile model.gguf
Quantifier en 4 bits
code
Bash
./quantize model.gguf model-q4_K_M.gguf q4_K_M
