Guide pratique (2025) â€” Quantization LLM sur H100 et alternatives (TRT-LLM, vLLM, GGUF)
1) Les bases : tenseurs, poids, activations, KV-cache

Un tenseur est un tableau multi-dimensionnel de nombres (scalaires, vecteurs, matrices, etc.). Dans un LLM, les poids (paramÃ¨tres du modÃ¨le) sont stockÃ©s sur disque puis chargÃ©s en VRAM sous forme de tenseurs, tandis que les activations dÃ©signent les rÃ©sultats intermÃ©diaires calculÃ©s pendant lâ€™infÃ©rence. La mÃ©moire KV-cache correspond aux tenseurs de clÃ©s (Keys) et valeurs (Values) de lâ€™attention, conservÃ©s au fil de la gÃ©nÃ©ration. Ce KV-cache accÃ©lÃ¨re lâ€™auto-rÃ©gression en Ã©vitant de recalculer tout lâ€™historique Ã  chaque nouveau token gÃ©nÃ©rÃ©. Sur les serveurs modernes, sa gestion est optimisÃ©e (ex. Paged Attention qui partitionne ce cache en pages, et in-flight batching qui regroupe des requÃªtes en vol). Par exemple, la bibliothÃ¨que open-source vLLM introduit PagedAttention pour stocker les Keys/Values en blocs non contigus, ce qui rÃ©duit le gaspillage de mÃ©moire Ã  moins de 4% (contre 60â€“80% de mÃ©moire KV perdue dans les systÃ¨mes classiques)
runpod.io
. En rÃ©sumÃ©, la VRAM requise Ã  lâ€™infÃ©rence se compose des poids du modÃ¨le, des activations temporaires, et du KV-cache dont la taille croÃ®t avec la longueur de contexte. Dans la pratique, les poids dominent souvent lâ€™empreinte mÃ©moire (environ 65% de la VRAM sur un modÃ¨le 13B), le KV-cache occupant ~30% (variable selon la longueur de sÃ©quence) et les activations une part minime
ar5iv.labs.arxiv.org
. Cela signifie que la taille minimale sur disque est surtout dictÃ©e par les poids, tandis quâ€™en RAM/VRAM lâ€™usage effectif dÃ©pend beaucoup du KV-cache pour les contextes longs.

Overflow / Underflow â€“ Rappel rapideÂ : un overflow survient quand une valeur dÃ©passe la plage reprÃ©sentable par le format numÃ©rique choisi et devient infinie, tandis quâ€™un underflow se produit lorsque la valeur est trop petite pour Ãªtre reprÃ©sentÃ©e (elle est alors arrondie Ã  zÃ©ro). Les formats Ã  Â«Â petite plageÂ Â» (faible dynamique), comme FP8 ou INT4/INT8, nÃ©cessitent des techniques de calibration pour Ã©viter ces problÃ¨mes. Par exemple, la mÃ©thode SmoothQuant ajuste lâ€™Ã©chelle des poids et activations pour Â«Â lisserÂ Â» les outliers (valeurs extrÃªmes) avant quantization
arxiv.org
.

2) Formats numÃ©riques : FP32, BF16, FP16, FP8, INT8, INT4

FP32 (float 32 bits)Â : format virgule flottante 32Â bits (23 bits de mantisse, 8 dâ€™exposant). Il offre une haute prÃ©cision et une large plage dynamique (~10^38), ce qui en fait la rÃ©fÃ©rence en entraÃ®nement, au prix dâ€™un coÃ»t mÃ©moire et calcul Ã©levÃ©.

BF16 (bfloat16)Â : format 16Â bits Ã  8 bits dâ€™exposant (mÃªme range que FP32) mais 7 bits de mantisse. Il conserve donc la mÃªme dynamique que FP32 tout en rÃ©duisant la prÃ©cision. TrÃ¨s utilisÃ© en entraÃ®nement mixte prÃ©cison sur TPU/GPU car il prÃ©serve lâ€™Ã©chelle des gradients.

FP16 (float16)Â : format 16Â bits IEEE (5 bits dâ€™exposant, 10 de mantisse). Sa plage de valeurs (~6.5Ã—10^4) est plus rÃ©duite que BF16, mais il offre plus de prÃ©cision significative. Câ€™est le standard en infÃ©rence GPU classique, alliant prÃ©cision suffisante et rapiditÃ© (Tensor Cores dÃ©diÃ©s sur GPU Ampere/Hopper).

FP8 (float8)Â : format flottant 8Â bits introduit avec NVIDIA Hopper (H100). Deux variantes existentÂ : E4M3 (4 bits dâ€™exposant, 3 de mantisse) et E5M2 (5 bits dâ€™exposant, 2 de mantisse). Elles offrent une dynamique beaucoup plus faible que FP16 (e.g. E4M3 reprÃ©sente des valeurs jusquâ€™Ã  Â±448 seulement, E5M2 jusquâ€™Ã  ~Â±5.7Ã—10^4)
docs.nvidia.com
. H100 prend en charge FP8 nativement via la Transformer Engine et ses Tensor Cores, ce qui permet de diviser par deux lâ€™empreinte mÃ©moire par rapport Ã  FP16 et dâ€™augmenter fortement le dÃ©bit, tout en maintenant une qualitÃ© proche de FP16 si le modÃ¨le est bien calibrÃ© (les valeurs extrÃªmes doivent Ãªtre traitÃ©es pour Ã©viter lâ€™instabilitÃ©). Par exemple, il faut souvent appliquer des recettes de quantization (per-tensor scaling, choix E4M3 vs E5M2 sur certaines couches) pour obtenir une infÃ©rence FP8 stable.

INT8 (entier 8 bits)Â : en quantization INT8 naive, chaque poids ou activation est rÃ©duit Ã  un entier codÃ© sur 8 bits (0â€“255 ou â€“128 Ã  127). Sans ajustement, cela entraÃ®nerait une perte importante dâ€™information (beaucoup de poids sont de petites dÃ©cimales proches de 0, qui deviendraient 0 une fois arrondis)
medium.com
. Câ€™est pourquoi on utilise des Ã©chelles (scales) par canal ou par tenseur pour mapper la plage de valeurs rÃ©elles aux 256 niveaux disponibles. Deux cas dâ€™usageÂ : (a) poids uniquement (on ne quantifie que les poids en INT8, les activations restant en FP16/BF16), ou (b) W8A8 (poids et activations int8). La mÃ©thode SmoothQuant a dÃ©montrÃ© quâ€™on pouvait obtenir un INT8 stable poids+activations en dÃ©placement la difficultÃ© de quantization des activations vers les poids par un simple rescaling prÃ©alable
arxiv.org
. Les GPU Ampere/Hopper disposent de Tensor Cores INT8 qui accÃ©lÃ¨rent ces calculs entiers. Un modÃ¨le W8A8 bien quantifiÃ© peut dÃ©livrer des performances proches de FP16 en Ã©tant deux fois plus lÃ©ger en mÃ©moire.

INT4 (entier 4 bits)Â : quantization ultra-agressive oÃ¹ chaque poids est reprÃ©sentÃ© sur 4 bits (16 niveaux seulement). En pratique, on nâ€™applique lâ€™INT4 quâ€™aux poids (weight-only) car quantifier les activations Ã  4 bits est extrÃªmement difficile sans rÃ©entraÃ®ner le modÃ¨le. Lâ€™INT4 est prisÃ© pour compresser les grands modÃ¨les et permettre leur exÃ©cution sur des hardwares contraints (GPU moyen, CPU, mobile) ou pour multiplier le nombre de sessions sur une VRAM donnÃ©e. Des techniques rÃ©centes comme AWQ ou GPTQ parviennent Ã  quantifier des LLM en 4 bits avec des pertes de qualitÃ© modestes, en utilisant par exemple des Ã©chelles par groupe de poids ou en sÃ©lectionnant quelques poids Â«Â critiquesÂ Â» Ã  garder en plus haute prÃ©cision
arxiv.org
arxiv.org
. Lâ€™INT4 nâ€™est pas directement accÃ©lÃ©rÃ© par le matÃ©riel (on simule du 4-bit en agrÃ©geant dans des mots 8Â bits/16Â bits), donc le gain de vitesse nâ€™est pas aussi Ã©levÃ© que le gain mÃ©moire, mais lâ€™empreinte rÃ©duite (4Ã— plus compacte que FP16) est un atout pour dÃ©ployer des modÃ¨les localement.

Pourquoi prÃ©ciser Â«Â poids + activationsÂ Â»Â ? De nombreuses mÃ©thodes de quantization ne compressent que les poids du modÃ¨le, car ce sont eux qui dÃ©terminent la taille du modÃ¨le sur disque et en mÃ©moire. Toutefois, mÃªme si les poids sont quantifiÃ©s en INT4/8, le calcul des activations lors de lâ€™infÃ©rence se fait souvent en FP16, ce qui limite le gain de vitesse. Passer en W8A8 (poids et activations en 8Â bits) permet de tirer parti de la quantization sur lâ€™ensemble du calcul (GEMM, etc.), dâ€™oÃ¹ lâ€™intÃ©rÃªt des solutions comme SmoothQuant ou FP8 qui traitent aussi les activations. En revanche, quantifier les activations est dÃ©licat car leurs distributions varient avec les entrÃ©esÂ ; dâ€™oÃ¹ lâ€™importance de la calibration.

RÃ©sumÃ© des formats :

Format	Bits (total)	Exposant/Mantisse	Plage dynamique approx.	Utilisation typique et remarques
FP32	32 bits	8 exp, 23 mant	~1e-38 Ã  1e+38	Haute prÃ©cision (rÃ©fÃ©rence). EntraÃ®nement, calculs sensibles (somme de pertes, etc.).
BF16	16 bits	8 exp, 7 mant	~1e-38 Ã  1e+38 (mÃªme range FP32)	EntraÃ®nement mixte prÃ©cision (TPU/GPU), infÃ©rence. MÃªme dynamique que FP32 mais prÃ©cision rÃ©duite (mantisse courte).
FP16	16 bits	5 exp, 10 mant	~1e-4 Ã  6.5e+4	InfÃ©rence sur GPU (Tensor Cores). PrÃ©cision suffisante dans la plupart des cas, range plus limitÃ© que BF16.
FP8 E4M3	8 bits	4 exp, 3 mant	~1e-2 Ã  ~4.5e+2	InfÃ©rence GPU Hopper (H100). Faible prÃ©cision, range modÃ©rÃ©. UtilisÃ© pour poids/activations forward (prÃ©cision nÃ©cessaire)
docs.nvidia.com
. Calibration impÃ©rative (Transformer Engine).
FP8 E5M2	8 bits	5 exp, 2 mant	~1e-2 Ã  ~5.7e+4 (+âˆ)	UtilisÃ© plutÃ´t pour gradients/backward (plus grande dynamique, moins besoin de prÃ©cision)
docs.nvidia.com
. En infÃ©rence pure, sert pour KV-cache FP8 (si > E4M3).
INT8 (W8A8)	8 bits	(entier pur)	256 valeurs (Ã©chelle configurable)	InfÃ©rence quantifiÃ©e poids + activations. Requiert calibration (ex. SmoothQuant) pour Ã©viter saturation
arxiv.org
. SupportÃ© sur GPU (Tensor Cores INT8) et CPU (SIMD int8).
INT4 (poids)	4 bits	(entier pur)	16 valeurs (par poids ou groupe)	Compression extrÃªme des poids (taille Ã·4 vs FP16). LÃ©gÃ¨re dÃ©gradation de style/cohÃ©rence possible si calibration approximative. UtilisÃ© via AWQ, GPTQâ€¦ Pas de support matÃ©riel natif (calcul via int8 simulÃ©).
3) H100 et FP8 : ce qui change

La gÃ©nÃ©ration Hopper (GPU NVIDIA H100) introduit des Tensor Cores prenant en charge directement le FP8, accompagnÃ©s de la Transformer Engine (TE) qui gÃ¨re automatiquement le passage FP16â†”FP8 selon un â€œrecipeâ€ optimal. Lâ€™intÃ©rÃªt principal est de doubler le dÃ©bit et rÃ©duire de moitiÃ© la mÃ©moire par rapport Ã  du FP16, pour une perte de qualitÃ© minime si la quantization est bien calibrÃ©e. Des benchmarks officiels montrent quâ€™un H100 exÃ©cutant un modÃ¨le en FP8 dÃ©passe largement un A100 en FP16 â€“ jusquâ€™Ã  Ã—4,6 de throughput en plus, et une latence du premier token ~4,4Ã— plus faible sur Llama-2
nvidia.github.io
.

Comparaison du throughput maximal de TensorRT-LLM sur H100 vs A100. La figure ci-dessus montre le dÃ©bit (tokens/s) obtenu avec TensorRT-LLM sur un GPU H100 (barres vertes, calcul en FP8) comparÃ© Ã  un A100 (barres noires, FP16) pour diffÃ©rents modÃ¨les et tailles de contexte. On observe par exemple un gain de Ã—4,6 sur GPT-JÂ 6B (contexte 2048 tokens) et des accÃ©lÃ©rations de lâ€™ordre de Ã—3â€“4 sur LlamaÂ 2 7B, confirmant lâ€™avantage majeur du FP8 sur H100 en termes de dÃ©bit
nvidia.github.io
. Ces gains sâ€™accompagnent de latences sensiblement rÃ©duitesÂ : en mode haute performance (beaucoup de requÃªtes parallÃ¨les), H100 FP8 maintient ~100Â ms de latence pour le 1er token contre ~480Â ms sur A100 FP16
nvidia.github.io
. En mode basse latence (batch 1), H100 peut descendre sous les 10Â ms pour le 1er token grÃ¢ce Ã  FP8. En pratique, sur un serveur 2Ã—H100, le FP8 devient le sweet spot optimisant Ã  la fois la qualitÃ©, la latence et la VRAM utilisÃ©e.

Le TensorRT-LLM de NVIDIA (voir section 6) exploite pleinement ces nouveautÃ©s du H100. Il intÃ¨gre en effet le support FP8 natif, lâ€™in-flight batching (regroupement de requÃªtes pour maximiser le remplissage GPU) et le paged KV-cache pour gÃ©rer la mÃ©moire attention de faÃ§on optimale
developer.nvidia.com
. RÃ©sultatÂ : sur H100, un modÃ¨le exÃ©cutÃ© en FP8 atteint des dÃ©bits inÃ©dits, souvent 3â€“5Ã— supÃ©rieurs Ã  la gÃ©nÃ©ration prÃ©cÃ©dente, tout en conservant une qualitÃ© de gÃ©nÃ©ration pratiquement inchangÃ©e. Par prÃ©caution, certains dÃ©ploient un modÃ¨le FP8 en production avec un second profil FP16 en parallÃ¨le pour comparer la qualitÃ©, mais les retours indiquent que les diffÃ©rences sont nÃ©gligeables si la calibration FP8 est bien faite (exÂ : plus de 99% de la performance dâ€™un modÃ¨le FP16 est prÃ©servÃ©e en FP8 dans vLLM dâ€™aprÃ¨s des Ã©valuations standard
developers.redhat.com
).

4) Le KV-cache : FP16 vs FP8

Par dÃ©faut, le KV-cache (les clÃ©s/valeurs de lâ€™attention) est maintenu en FP16 lors de lâ€™infÃ©rence, ce qui assure une fidÃ©litÃ© maximale mais consomme beaucoup de VRAM â€“ environ deux fois plus quâ€™en FP8. Sur un contexte de 16k tokens, le KV-cache FP16 dâ€™un LLM 30B peut occuper plusieurs Go de VRAM. Passer le KV-cache en FP8 divise par deux cette empreinte, permettant dâ€™augmenter la longueur de contexte et/ou le nombre de sessions servies simultanÃ©ment pour une mÃªme mÃ©moire. La contrepartie est un trÃ¨s lÃ©ger risque de perte de qualitÃ© (puisque les valeurs dâ€™attention sont un peu moins prÃ©cises), mais en pratique les tests montrent un impact quasi nul avec du FP8 calibrÃ© sur H100
developers.redhat.com
.

Les serveurs de gÃ©nÃ©ration modernes offrent souvent lâ€™option de choisir le dtype du KV-cache indÃ©pendamment de celui des poids. On peut par exemple utiliser un modÃ¨le en FP16 tout en stockant le KV-cache en FP8 pour Ã©conomiser de la VRAM, ou inversement garder un KV-cache en FP16 avec un modÃ¨le quantifiÃ© pour maximiser la fidÃ©litÃ© des attentions. Ce mÃ©lange des prÃ©cisions est tout Ã  fait possible et contrÃ´lÃ© par des flags (exÂ : --kv-cache-dtype fp8 dans vLLM, --use_fp8_kv_cache dans TensorRT-LLM). Lâ€™approche dÃ©pend de la marge mÃ©moire dont on disposeÂ : si la VRAM est le facteur limitant, mettre le KV en FP8 est un quick win pour augmenter le contexte servable. Ã€ lâ€™extrÃªme, certains explorent mÃªme le KV-cache en 4 bits (int4) pour les trÃ¨s longs contextes, mais câ€™est encore expÃ©rimental.

Par ailleurs, des algorithmes comme PagedAttention (voir section 6B) amÃ©liorent lâ€™usage du KV-cache indÃ©pendamment du dtype, en le dÃ©coupant en pages plus petites pour Ã©viter la fragmentation. Cette paged KV-cache permet de rÃ©allouer finement la mÃ©moire KV et de la partager entre requÃªtes, ce qui rÃ©duit drastiquement le gÃ¢chis (moins de zones inutilisÃ©es)
blog.vllm.ai
blog.vllm.ai
. En pratique, sur un serveur multi-utilisateurs, combiner KV-cache FP8 et PagedAttention offre le meilleur des deux mondesÂ : un KV-cache compact et gÃ©rÃ© sans perte, pour servir plus de contextes longs simultanÃ©ment.

En rÃ©sumÃ© : garder le KV-cache en FP16 assure la fidÃ©litÃ© maximale mais consomme beaucoup de VRAM, tandis que le passer en FP8 libÃ¨re ~50% de mÃ©moire KV pour un impact nÃ©gligeable sur la qualitÃ© si bien calibrÃ©. Cette optimisation est particuliÃ¨rement utile au-delÃ  de 8kâ€“16k tokens de contexte, ou pour hÃ©berger de nombreux chats Ã  la fois.

5) MÃ©thodes de quantization clÃ©s

Plusieurs mÃ©thodes ont Ã©mergÃ© pour quantifier les LLMs de faÃ§on efficaceÂ :

SmoothQuant (INT8 W8A8, post-training) â€“ Il sâ€™agit dâ€™une mÃ©thode de post-training quantization (PTQ) introduite en 2022-2023, permettant de quantifier en 8Â bits Ã  la fois les poids et les activations. Lâ€™idÃ©e centrale est de lisser les outliers dâ€™activation en transfÃ©rant une partie de leur amplitude vers les poids, via un simple rescaling proportionnel
arxiv.org
. En effet, les auteurs ont constatÃ© que les poids dâ€™un LLM sont globalement faciles Ã  quantifier, alors que certaines activations prÃ©sentent des pics (â€œoutliersâ€) rendant la quantization Ã  8 bits difficile. SmoothQuant calcule pour chaque couche un facteur dâ€™Ã©chelle qui, appliquÃ© aux poids, Ã©quilibre leur distribution vs celle des activations, de sorte que quantifier le tout en INT8 provoque beaucoup moins dâ€™erreurs. Câ€™est une approche entiÃ¨rement sans rÃ©-entraÃ®nement (pas de fine-tuning nÃ©cessaire), applicable Ã  nâ€™importe quel modÃ¨le. SmoothQuant a dÃ©montrÃ© quâ€™on pouvait quantifier en 8-bit un modÃ¨le jusquâ€™Ã  530Â milliards de paramÃ¨tres avec une perte de prÃ©cision nÃ©gligeable
arxiv.org
. Les gains mesurÃ©s sont jusquâ€™Ã  ~1.5Ã— dâ€™accÃ©lÃ©ration et 2Ã— de rÃ©duction mÃ©moire, le tout sans dÃ©grader la qualitÃ© (Ã  0.3â€“0.5Â pp prÃ¨s sur les benchmarks). SmoothQuant a Ã©tÃ© intÃ©grÃ© Ã  de nombreux outils (exÂ : Intel Neural Compressor, MMRazor) et sert de base aux implÃ©mentations INT8 sur H100.

AWQ (Activation-aware Weight Quantization, INT4 poids-seul) â€“ Cette mÃ©thode (Lin et al., MLSys 2024) vise Ã  quantifier les poids en 4 bits de maniÃ¨re robuste, en se basant sur lâ€™analyse des activations. AWQ fait lâ€™hypothÃ¨se que seuls ~1% des canaux de poids sont vraiment critiques pour la performance, et que ces canaux peuvent Ãªtre identifiÃ©s via leur distribution dâ€™activation
arxiv.org
. ConcrÃ¨tement, on exÃ©cute quelques donnÃ©es dâ€™Ã©talonnage Ã  travers le modÃ¨le pour repÃ©rer les salient weights (poids dont lâ€™activation absolue est Ã©levÃ©e), puis on protÃ¨ge ces 1% de poids (en les quantifiant sur 8 bits ou en les laissant en FP16), tandis que tous les autres 99% sont quantifiÃ©s en 4 bits. De plus, AWQ applique un scaling particulier sur ces canaux importants pour rÃ©duire encore lâ€™erreur de quantization
arxiv.org
. Lâ€™intÃ©rÃªt est quâ€™il nâ€™y a pas de calibration fine par backpropagation, et donc pas de risque de sur-ajustement sur le set de calibrationÂ : AWQ gÃ©nÃ©ralise bien Ã  dâ€™autres domaines (code, math, etc.)
arxiv.org
. Les rÃ©sultats montrent que AWQ surpasse les mÃ©thodes antÃ©rieures sur du 4-bit, et a mÃªme permis pour la premiÃ¨re fois de quantifier correctement des LLM instruction-tuned et multi-modaux en 4 bits
arxiv.org
. Cette mÃ©thode a reÃ§u le Best Paper Award Ã  MLSys 2024. En pratique, AWQ est utilisÃ© pour gÃ©nÃ©rer des poids 4-bit de haute qualitÃ© (exÂ : les modÃ¨les 4-bit publiÃ©s par AWS, et certains â€œGGUF Q4_K_Mâ€ en sont inspirÃ©s).

GPTQ (INT3/INT4 poids-seul) â€“ ProposÃ©e fin 2022
arxiv.org
, GPTQ est une mÃ©thode de quantization one-shot (en une passe) qui utilise des informations de second-ordre (approximation Hessienne) pour minimiser la perte de prÃ©cision due Ã  la quantization des poids. PlutÃ´t que de quantifier bÃªtement chaque poids indÃ©pendamment, GPTQ optimise bloc par bloc en calculant lâ€™erreur induite et en la compensant sur les poids restants du bloc (dâ€™oÃ¹ le nom GPT Quantization car initialement testÃ© sur GPT-3). Lâ€™algorithme parvient Ã  quantifier des modÃ¨les GPT/LLM jusquâ€™Ã  175Â Md de paramÃ¨tres en 3 ou 4 bits par poids, en quelques heures sur un seul GPU
arxiv.org
, avec une perte de performance quasi nulle par rapport au modÃ¨le FP16 original. Par exemple, ils montrent quâ€™on peut quantifier GPT-NeoX-20B en 3 bits sans dÃ©gradation significative, et GPT3 175B en 4 bits en ~4h
arxiv.org
. GPTQ double le taux de compression par rapport aux mÃ©thodes one-shot prÃ©cÃ©dentes tout en prÃ©servant mieux lâ€™exactitude
arxiv.org
. Cela a Ã©tÃ© rapidement adoptÃ© dans la communautÃ© open-sourceÂ : de nombreux LLM quantifiÃ©s partagÃ©s sur HuggingFace utilisent GPTQ (fichiers .pt, .safetensors avec gptq), et des projets comme AutoGPTQ, Transformers, ExLlama ont des backends optimisÃ©s pour ces poids GPTQ 4-bit. GPTQ reste une rÃ©fÃ©rence pour obtenir une excellente qualitÃ© en 3â€“4 bits sans se compliquer la vie.

LLM.int8 (bitsandbytes) â€“ Avant lâ€™essor de SmoothQuant et consorts, Tim Dettmers et al. (NeurIPS 2022) ont proposÃ© GPT3.int8() alias LLM.int8(), une approche astucieuse pour faire de lâ€™INT8 sans perte sur des modÃ¨les comme GPT-3
arxiv.org
arxiv.org
. Leur observationÂ : les poids dâ€™un transformeur prÃ©sentent des outlier features (quelques dimensions activÃ©es fortement) qui posent problÃ¨me si on applique un seul scale int8 sur tout un tenseur. Leur solutionÂ : utiliser une quantization vectorielle (par groupe de neurones) avec un facteur dâ€™Ã©chelle par colonne de matrice
arxiv.org
, pour quantifier 99.9% des opÃ©rations en int8, et isoler les outliers dans une multiplication sÃ©parÃ©e en 16 bits
arxiv.org
. ConcrÃ¨tement, on fait du GEMM 8-bit sur la majeure partie des dimensions, et les 0.1% de dimensions les plus â€œdangereusesâ€ (outliers) sont traitÃ©es en FP16 en parallÃ¨le. Au final, 99.9% des opÃ©rations sont int8, ce qui divise par ~2 la mÃ©moire dâ€™infÃ©rence sans perte de perf mesurable
arxiv.org
. LLM.int8 a Ã©tÃ© implÃ©mentÃ© dans la bibliothÃ¨que bitsandbytes, trÃ¨s utilisÃ©e en 2022-2023 pour charger des modÃ¨les 8-bit sur GPU peu VRAM. Cependant, cette mÃ©thode weight-only nâ€™accÃ©lÃ¨re pas vraiment le calcul (elle lâ€™allÃ¨ge juste en mÃ©moire), et sâ€™avÃ¨re moins stable que W8A8 ou FP8 sur H100. En pratique sur H100, on lui prÃ©fÃ¨rera SmoothQuant ou FP8 qui exploitent pleinement les Tensor Cores, mais LLM.int8 reste utile sur du hardware ne supportant pas W8A8 (exÂ : A100 oÃ¹ on veut Ã©viter de calibrer).

6) Piles logicielles : TRT-LLM vs vLLM vs llama.cpp/GGUF
A) TensorRT-LLM (NVIDIA) â€“ Il sâ€™agit dâ€™un nouveau runtime/compilateur optimisÃ© par NVIDIA pour lâ€™infÃ©rence LLM. TensorRT-LLM (TRT-LLM) prend un modÃ¨le HuggingFace et le compile en un moteur binaire ultra-performant spÃ©cifique Ã  votre GPU (similaire Ã  TensorRT classique mais orientÃ© LLM). Ses points forts : support FP8 natif sur H100, support de lâ€™INT8 (SmoothQuant) et INT4 (AWQ) en compilation, utilisation avancÃ©e du matÃ©riel (Tensor Cores, chargement asynchroneâ€¦), le tout avec in-flight batching intÃ©grÃ© (pour gÃ©rer efficacement des requÃªtes parallÃ¨les de longueurs variÃ©es) et paged KV-cache (gestion optimisÃ©e de la mÃ©moire attention, rÃ©utilisation inter-requÃªtes)
developer.nvidia.com
. TRT-LLM supporte en outre le multi-GPU (Tensor Parallelism, Pipeline Parallelism) et des fonctionnalitÃ©s comme le streaming de tokens. En pratique, sur H100, câ€™est la pile offrant les meilleures latences et throughput absolus, au prix dâ€™une moindre flexibilitÃ© (il faut convertir/compiler le modÃ¨le). NVIDIA fournit des quick-starts et conteneurs NGC facilitant son dÃ©ploiement. La dÃ©marche typique : exporter un checkpoint HF en format TRT-LLM, puis builder lâ€™engine avec les options souhaitÃ©es (--use_fp8 ou --quantize int8 etc.), enfin lancer le serveur trtllm-serve. Une fois compilÃ©, le moteur peut Ãªtre invoquÃ© via une API C++/Python haute performance.
*Exemple dâ€™usageÂ :* Sur un serveur 2Ã—H100, on peut convertir Llama2 70B HF en engine TensorRT-LLM FP8 en quelques minutes, puis servir des requÃªtes gRPC avec une latence <10Â ms tokÃ©nisation comprise. NVIDIA annonce sur Llama2 70B ~4.6Ã— plus de throughput quâ€™A100, et ~8Ã— en combinant H100+TRT-LLM vs A100 sans TRT:contentReference[oaicite:36]{index=36}:contentReference[oaicite:37]{index=37}. Autrement dit, TRT-LLM **explose les scores** sur H100 grÃ¢ce Ã  la compilation spÃ©cialisÃ©e et FP8.

*LimitesÂ :* TRT-LLM est focalisÃ© NVIDIAÂ GPU â€“ il ne tourne que sur GPUs NVIDIA avec CUDAÂ >=11.x. Il ne supporte pas toutes les architectures de modÃ¨le exotiques dÃ¨s leur sortie (il se synchronise sur les principaux modÃ¨les open-source, mais il peut y avoir du dÃ©lai). Par exemple, un Llama2 avec certaines modifications pourrait nÃ©cessiter une mise Ã  jour du parser TRT-LLM. De plus, un moteur compilÃ© est spÃ©cifique : un engine H100 ne fonctionnera pas sur A100, et vice versa, et nâ€™est pas *portable* hors de TRT (on ne peut pas le recharger dans PyTorch). Il faut donc **garder le checkpoint HF original** en parallÃ¨le au cas oÃ¹ lâ€™on veuille utiliser une autre solution. MalgrÃ© cela, TRT-LLM Ã©tant open-source depuis fin 2023:contentReference[oaicite:38]{index=38}, on voit la communautÃ© lâ€™adapter progressivement et ajouter le support de nouveaux modÃ¨les (exÂ : Mistral 7B supportÃ© peu aprÃ¨s sa sortie).

B) vLLM (Open-Source) â€“ vLLM est un serveur dâ€™infÃ©rence LLM open-source dÃ©veloppÃ© par UCÂ Berkeley, pensÃ© pour la performance optimale tout en restant flexible (intÃ©gration Python). Sa particularitÃ© est lâ€™algorithme PagedAttention (voir papier SOSP 2023) qui gÃ¨re le KV-cache de faÃ§on quasi optimale en termes de mÃ©moire. ConcrÃ¨tement, vLLM alloue le KV-cache en pages de taille fixe au lieu dâ€™un gros tensor contigu, et utilise une table de correspondance pour assembler les pages correspondant Ã  chaque requÃªte
blog.vllm.ai
. Ainsi, la mÃ©moire nâ€™est presque plus fragmentÃ©eÂ : <4% de waste mesurÃ©, au lieu de 60â€“80% sur HuggingFace Transformers ou FasterTransformer
runpod.io
. Cela permet de servir beaucoup plus de requÃªtes en parallÃ¨le sans saturer la VRAM, surtout sur des contextes longs. vLLM a montrÃ© des throughput jusquâ€™Ã  24Ã— supÃ©rieurs Ã  HF Transformers et ~3Ã— supÃ©rieurs Ã  TGI
blog.vllm.ai
 dans ses benchmarks, grÃ¢ce Ã  cette gestion mÃ©moire et Ã  un scheduler optimisÃ©.
En termes de **quantization**, vLLM supporte depuis la v0.5 le **FP8 (W8A8)** sur GPUs rÃ©cents (H100, mais aussi initialement MI300x cÃ´tÃ© AMD):contentReference[oaicite:42]{index=42}. Il supporte Ã©galement lâ€™**INT8 W8A8** (SmoothQuant) et le chargement de poids 4-bit (AWQ, GPTQ) via des formats comme AWQ (.pt) ou GGML/GGUF. La commande `vllm serve` propose un argument `--quantization` pour spÃ©cifier `fp8`, `int8`, etc., ainsi que `--kv-cache-dtype` pour choisir FP8/FP16 sur le KV. CÃ´tÃ© intÃ©gration, vLLM fournit une API Python trÃ¨s simple (similaire Ã  `generate` de HuggingFace, mais en serveur multi-clients). On peut donc lâ€™utiliser facilement dans un pipeline dâ€™application. Autre avantageÂ : vLLM intÃ¨gre naturellement du **batching dynamique** (il regroupe les requÃªtes reÃ§ues Ã  la volÃ©e tant que possible) et supporte le *streaming* de la rÃ©ponse token par token.

En pratique, vLLM est idÃ©al si on veut une solution 100% open-source, multi-plateformes, tout en ayant des performances de haut niveau. Par exemple, sur un mÃªme H100, vLLM en FP8 aura un throughput lÃ©gÃ¨rement infÃ©rieur Ã  TRT-LLM FP8 (puisque TRT compile tout en kernels C++ optimisÃ©s), mais vLLM offrira plus de souplesse (changement de modÃ¨le Ã  la volÃ©e, support multi-GPU moins rigide, etc.). Sur des contextes trÃ¨s longs ou des charges multi-users imprÃ©visibles, PagedAttention peut mÃªme donner lâ€™avantage Ã  vLLM en efficacitÃ©. Le choix entre TRT-LLM et vLLM se fait donc entre **performance maximale absolue** (TRT) et **flexibilitÃ© OSS** (vLLM), sachant que vLLM est dÃ©jÃ  extrÃªmement performant comparÃ© aux serveurs traditionnels.

C) llama.cpp / GGUF (CPU & autres) â€“ llama.cpp dÃ©signe Ã  lâ€™origine une implÃ©mentation C++ minimaliste pour exÃ©cuter LLaMA sur CPU. Depuis, lâ€™Ã©cosystÃ¨me sâ€™est Ã©tendu pour supporter de nombreux modÃ¨les et quantizations, avec le format GGUF (successeur de GGML) pour stocker les poids quantifiÃ©s. Les atouts de llama.cppÂ : câ€™est multiplateforme (CPU, GPU non-CUDA, Apple Siliconâ€¦), trÃ¨s facile Ã  dÃ©ployer (un exÃ©cutable unique), et il existe une multitude de variants/UI (text-generation-webui, etc.) lâ€™utilisant. Il prend en charge des quantizations spÃ©cialisÃ©es notÃ©es par des suffixes (Q4_0, Q4_K_M, Q5_1, Q8_0, etc.). Par exemple, Q8_0 correspond Ã  une quantization 8-bit non groupÃ©e (poids sur 8 bits, sans calibration particuliÃ¨re) â€“ en pratique proche dâ€™une compression sans perte sur les poids. Q4_K_M est un format 4-bit avec quantization par groupe (K pour groupwise) et prÃ©cision Medium (M), offrant un bon compromis entre qualitÃ© et taille
medium.com
. Ces formats proviennent des travaux comme GPTQ, AWQ, et de nombreuses expÃ©rimentations communautaires. On peut convertir un modÃ¨le HF en GGUF quantifiÃ© via des outils (exÂ : convert-hf-to-gguf.py + quantize fournis dans llama.cpp
qwen.readthedocs.io
qwen.readthedocs.io
). Une fois en GGUF, le modÃ¨le peut Ãªtre exÃ©cutÃ© via llama.cpp ou des variantes comme text-gen-webui, parfois mÃªme chargÃ©s dans des runtimes spÃ©cifiques (exÂ : accÃ©lÃ©ration GPU via exllama pour Q4).
**Utilisation typiqueÂ :** llama.cpp/GGUF est parfait pour le *prototypage* local, le dÃ©ploiement sur des machines sans GPU puissant, ou le partage communautaire de modÃ¨les quantifiÃ©s. Par exemple, on peut faire tourner un LLM 30B 4-bit sur un laptop CPU haut de gamme, certes lentement mais sans dÃ©pendre de CUDA. Sur GPU, llama.cpp utilise plutÃ´t la VRAM via CUDA ou Metal (accÃ©lÃ©ration partielle), mais reste moins optimisÃ© que TRT-LLM ou mÃªme que HuggingFace Transformers sur GPU (puisquâ€™il nâ€™utilise pas les Tensor Cores trÃ¨s efficacement). Donc pour un H100 on favorisera TRT-LLM ou vLLM, mais pour un *edge server* ou une machine hÃ©tÃ©rogÃ¨ne, llama.cpp offre une universalitÃ© apprÃ©ciable.

**CompatibilitÃ©Â :** Un point important est que les engines TRT-LLM ou mÃªme les modÃ¨les vLLM ne sont pas interopÃ©rables avec llama.cpp, et vice-versa. Un modÃ¨le GGUF doit Ãªtre reconverti pour Ãªtre servi en vLLM ou TRT, ce qui nÃ©cessite de repartir du checkpoint HF initial le plus souvent. Il est donc recommandÃ© de **conserver le checkpoint HuggingFace** dâ€™origine de chaque modÃ¨le, et de ne considÃ©rer les conversions (engine TensorRT, quant GGUFâ€¦) que comme des *builds* dÃ©rivÃ©s pour un usage spÃ©cifique.

7) Â«Â 8-bitÂ Â» sous vLLMÂ : que choisirÂ ?

Si vous utilisez vLLM et que vous souhaitez rÃ©duire la prÃ©cision pour gagner en vitesse/mÃ©moire, deux options 8-bit sâ€™offrent Ã  vousÂ : FP8 ou INT8. Sur matÃ©riel NVIDIA H100, la recommandation est gÃ©nÃ©ralement dâ€™opter pour FP8 (W8A8), car câ€™est ce qui offre le meilleur compromis performance/qualitÃ©. Par exemple, en FP8, un H100 peut diviser par deux la latence inter-token par rapport Ã  FP16
developers.redhat.com
. Pour activer ce mode dans vLLMÂ :

vllm serve $MODEL_ID \
  --quantization fp8 \
  --kv-cache-dtype fp8 \
  --max-model-len 16384


Avec ces paramÃ¨tres, vLLM quantifiera Ã  la volÃ©e le modÃ¨le en 8-bit flottant (poids et activations) et stockera le KV-cache en FP8, tout en fixant une longueur de contexte max de 16k. Il utilise pour cela les Tensor Cores FP8 du GPU (ou les unitÃ©s AI correspondantes si AMD MI300x). La dÃ©gradation de qualitÃ© est minime si le modÃ¨le est de taille raisonnable et a Ã©tÃ© calibrÃ© correctement (la plupart du temps on peut quantifier un LLM 13B/70B en FP8 sans changement notable dans ses gÃ©nÃ©rations
developers.redhat.com
).

Lâ€™alternative est lâ€™INT8 (W8A8), câ€™est-Ã -dire la quantization 8-bit entiÃ¨re de SmoothQuant. Celle-ci est utile si, pour une raison ou une autre, vous ne souhaitez pas du FP8 (par ex. pas de GPU Hopper). On activerait alorsÂ :

vllm serve $MODEL_ID \
  --quantization int8 \
  --kv-cache-dtype fp8 \
  --max-model-len 16384


Ici on quantifie les poids+acts en INT8. SmoothQuant Ã©tant intÃ©grÃ©, la robustesse est normalement assurÃ©e â€“ lÃ  encore, la qualitÃ© devrait rester trÃ¨s proche du FP16 dâ€™origine sur les tests usuels. Ã€ noterÂ : vLLM supporte aussi le chargement de modÃ¨les dÃ©jÃ  quantifiÃ©s (exÂ : --quantization awq pour du 4-bit AWQ), mais en pratique on obtiendra de meilleures perfs en quantifiant Ã  la volÃ©e en int8 ou fp8, car cela permet dâ€™utiliser les Tensor Cores 8-bit.

Une source de confusion peut venir de bitsandbytesÂ : dans HuggingFace Transformers, on utilisait load_in_8bit=True (bitsandbytes LLM.int8) pour charger un modÃ¨le en 8-bit poids seulement. Ce nâ€™est pas la mÃªme chose que le --quantization int8 de vLLM, qui lui signifie W8A8 (poids et acts 8-bit). Bitsandbytes nâ€™est pas nÃ©cessaire avec vLLM, celui-ci gÃ¨re nativement le 8-bit complet. Par ailleurs, bitsandbytes nâ€™apporte pas dâ€™accÃ©lÃ©rationÂ : câ€™Ã©tait surtout utile sur les GPU 16/32 Go pour caser des grands modÃ¨les en RAM. Sur H100, FP8 ou INT8 via TensorRT/vLLM seront nettement plus efficaces.

8) Recommandations concrÃ¨tes (cas 2Ã—H100)

Supposons une machine dual-GPU H100 80Â Go sur laquelle on veut dÃ©ployer un ou plusieurs LLM de ~70Â milliards de paramÃ¨tres avec contexte long. ObjectifÂ : maximiser le throughput et la densitÃ© de sessions tout en minimisant les rÃ©gressions de style/cohÃ©rence du modÃ¨le (on veut que Ã§a reste presque aussi bon quâ€™en FP16). Voici quelques recommandations pratiquesÂ :

FP8 de bout en bout â€“ Si votre modÃ¨le cible supporte bien FP8, câ€™est lâ€™option Ã  privilÃ©gier sur H100. Câ€™est-Ã -dire poids et activations en FP8, et KV-cache en FP8. La qualitÃ© sera ~Ã©quivalente Ã  FP16 dâ€™aprÃ¨s les Ã©valuations (99%+ conservÃ©)
developers.redhat.com
, tandis que la vitesse et lâ€™empreinte mÃ©moire seront bien meilleures. En TensorRT-LLM, activer --use_fp8 --use_fp8_kv_cache permet cela (sous rÃ©serve dâ€™avoir un GPU SM89). En vLLM, utiliser --quantization fp8 --kv-cache-dtype fp8. Cette config dÃ©livre gÃ©nÃ©ralement le top en qualitÃ©/perf sur H100.

INT8 SmoothQuant (W8A8) â€“ Si, pour des raisons de standardisation ou de prudence, vous prÃ©fÃ©rez rester en Â«Â entiers 8-bitÂ Â», alors une quantization SmoothQuant 8-bit est idÃ©ale. Elle est trÃ¨s stable (peu ou pas de perte sur des modÃ¨les bien connus
arxiv.org
) et bÃ©nÃ©ficie aussi de lâ€™accÃ©lÃ©ration Tensor Core int8. Par rapport Ã  FP8, lâ€™inconvÃ©nient potentiel est une lÃ©gÃ¨re perte de perf (INT8 vs FP8, sur H100 le FP8 est un peu plus rapide) et la nÃ©cessitÃ© dâ€™un petit calibrage des Ã©chelles SmoothQuant (quoique câ€™est gÃ©nÃ©ralement fourni ou trivial Ã  faire). En bref, INT8 W8A8 est un choix â€œsÃ»râ€ et universel si FP8 pose problÃ¨me.

INT4 AWQ (+ KV FP8) â€“ Pour maximiser le nombre de modÃ¨les/sessions dans la VRAM, on peut descendre Ã  4 bits sur les poids. Une approche Ã©prouvÃ©e est AWQ 4-bit sur les poids, combinÃ©e Ã  un KV-cache en FP8. On obtient ainsi un modÃ¨le extrÃªmement compact (taille divisÃ©e par 4 vs FP16, donc un 70B tient dans ~40Â Go) tout en conservant les activations en 16 bits pour le calcul. La qualitÃ© en prend un lÃ©ger coup (quelques points de perplexitÃ© en plus, style parfois un peu moins fin), mais pour beaucoup dâ€™usages Ã§a reste acceptable â€“ on parle de lÃ©gÃ¨re rÃ©gression de cohÃ©rence, pas dâ€™un effondrement. AWQ ayant dÃ©montrÃ© une excellente gÃ©nÃ©ralisation, le modÃ¨le 4-bit se comportera correctement sur des entrÃ©es variÃ©es, avec peut-Ãªtre un peu plus de rÃ©pÃ©titions ou de rÃ©ponses stÃ©rÃ©otypÃ©es. Si la prioritÃ© est de pouvoir faire tourner 2 instances de modÃ¨le sur 2Ã—H100 (par ex. deux 70B), lâ€™INT4 est quasiment le seul moyen. Dans ce cas, il faudra bien tester sur quelques prompts sensibles pour vÃ©rifier que la dÃ©gradation de qualitÃ© reste tolÃ©rable dans votre cas dâ€™usage.

RÃ¨gles simples de validationÂ : Quelle que soit la quantization choisie, il est conseillÃ© de calibrer et tester le modÃ¨le sur un jeu de prompts reprÃ©sentatif de lâ€™usage rÃ©el. Par exemple, si vos utilisateurs font du dialogue en franÃ§ais sur 4â€“8Â k tokens, prÃ©parez ~20â€“50 prompts de ce type (questions ouvertes, suivies de rÃ©ponses attendues) et comparez les outputs du modÃ¨le FP16 vs quantifiÃ© (FP8/INT8/INT4) en blind test. Outre le jugement humain, on peut regarder des mÃ©triques automatiques (perplexitÃ© sur un corpus, similaritÃ© dâ€™embeddings, mesure de diversitÃ© distinct-n, etc., ainsi que des taux de refus ou dâ€™hallucination si câ€™est critique pour vous). Ces tests permettront de repÃ©rer si, par exemple, le modÃ¨le quantifiÃ© a plus tendance Ã  divaguer ou Ã  rÃ©pÃ©ter des phrases. GÃ©nÃ©ralement, en ajustant lÃ©gÃ¨rement les paramÃ¨tres de dÃ©codage on peut compenserÂ : p.ex. augmenter le repetition_penalty (de 1.1 Ã  1.2) aide souvent un modÃ¨le quantifiÃ© Ã  Ã©viter le rambling. Pour des modÃ¨les multilingues, assurez-vous de tester dans les langues principales de lâ€™usage (un quant peut avoir un lÃ©ger biais vers lâ€™anglais si on ne fait pas gaffe, selon les outliers de certaines tokens). Enfin, pour le contexte long (â‰¥16k), prÃ©voyez impÃ©rativement le KV-cache en FP8 si la VRAM est juste, sinon vous risquez lâ€™OOM avant dâ€™atteindre la limite de tokens.

En rÃ©sumÃ©, sur 2Ã—H100, on pourra prÃ©parer deux profils par modÃ¨leÂ : un profil haute qualitÃ© (FP8 end-to-end) et un profil haute densitÃ© (INT4 ou INT8 selon besoin). Ensuite, en fonction de la charge, on utilise lâ€™un ou lâ€™autre. Par exemple, heures creusesÂ : on peut privilÃ©gier FP8 pour qualitÃ© optimaleÂ ; heures pleinesÂ : basculer en INT4 pour servir plus de requÃªtes simultanÃ©ment. Lâ€™important est dâ€™automatiser ces bascules proprement si on le fait (certains orchestrateurs peuvent allouer dynamiquement une version quantifiÃ©e du modÃ¨le selon lâ€™URL de requÃªte ou autre).

9) Pipelines type (dÃ©ploiement)

Voici diffÃ©rents pipelines et configurations courantes pour la mise en production de LLM quantifiÃ©sÂ :

Pipeline A â€” TRT-LLM FP8 (recommandÃ© sur H100)

Exporter le modÃ¨le HF au format TensorRT-LLMÂ :

python3 examples/llama/convert_checkpoint.py \
  --model_dir /models/YourModelHF \
  --output_dir /out/trtllm_ckpt \
  --dtype float16 --tp_size 2


(Cette Ã©tape transforme le checkpoint HuggingFace en un checkpoint TensorRT-LLM en FP16, avec ici un Tensor Parallelism TP=2 pour 2Â GPU.)

Builder le moteur TensorRT-LLM en FP8Â :

trtllm-build \
  --checkpoint_dir /out/trtllm_ckpt \
  --output_dir /out/engine_fp8_tp2 \
  --tp_size 2 --max_batch_size 16 \
  --max_input_len 16384 --max_output_len 1024 \
  --use_fp8 --use_fp8_kv_cache


Ici on compile le moteur avec quantization FP8 (poids+acts) et KV-cache FP8, pour batch jusquâ€™Ã  16 et contexte 16k. Le builder va optimiser le plan dâ€™exÃ©cution en fonction de ces contraintes.

Lancer le serveurÂ :

trtllm-serve --engine_dir /out/engine_fp8_tp2 --port 8000


Cela lance un serveur gRPC/HTTP local Ã©coutant sur le port 8000, prÃªt Ã  recevoir des requÃªtes de gÃ©nÃ©ration. Le serveur gÃ¨re le streaming, le batching dynamique, etc. (cf. docs NVIDIA).

ğŸ‘‰ RÃ©fÃ©rencesÂ : la documentation officielle de TRT-LLM (Overview, Quick Start) dÃ©taille ces Ã©tapes
developer.nvidia.com
nvidia.github.io
. En gÃ©nÃ©ral, ce pipeline FP8 offre la meilleure latence token et dÃ©bit par GPU sur H100.

Pipeline B â€” TRT-LLM INT8 SmoothQuant

Si on vise du 8-bit strict (pas de FP8), on peut utiliser le builder TRT-LLM en mode INT8. Il faut dâ€™abord calibrer SmoothQuant (soit utiliser leur script de calibration avec quelques donnÃ©es, soit charger un modÃ¨le dÃ©jÃ  smoothquantÃ©). EnsuiteÂ :

Export HF â†’ TRT-LLM checkpoint en FP16 (idem Ã©tape 1 ci-dessus).

Calibration SmoothQuantÂ : TRT-LLM fournit une option --quantize int8 lors du build, qui nÃ©cessite de pointer vers un dataset de calibration (quelques centaines de phrases). Il applique alors SmoothQuant en interne
arxiv.org
. Alternativement, on peut smoothquantiser le modÃ¨le hors-ligne (exÂ : script SmoothQuant.py du repo NVIDIA).

Build INT8Â :

trtllm-build ... --quantize int8 --use_fp8_kv_cache ...


(on recommande KV-cache en FP8 mÃªme si modÃ¨le en INT8, pour gagner de la VRAM).

Serve via trtllm-serve comme avant.

Ce pipeline donne un moteur 8-bit poids+acts. La qualitÃ© sera trÃ¨s proche du FP16 (SmoothQuant garantit peu de perte), le throughput un peu en-deÃ§Ã  du FP8 (mais tout de mÃªme meilleur que FP16). Câ€™est utile si lâ€™on veut absolument Ã©viter FP8 ou si le modÃ¨le se quantifie mal en FP8 pour une raison quelconque. Les publications originales de SmoothQuant fournissent plus de dÃ©tails sur la calibration utilisÃ©e
arxiv.org
arxiv.org
.

Pipeline C â€” vLLM FP8 ou INT8

Cette approche est ultra-simpleÂ : pas de build lourd, on utilise vLLM directement avec le modÃ¨le HuggingFace. Exemple en FP8Â :

vllm serve mistralai/Mistral-7B-Instruct \
    --quantization fp8 \
    --kv-cache-dtype fp8 \
    --max-model-len 8192


Cela va charger le modÃ¨le en FP16 puis appliquer la quantization FP8 Ã  la volÃ©e (avec support Hopper requis). On pourrait choisir int8 Ã  la place. Lâ€™argument --max-model-len fixe le contexte max (important pour allouer le KV-cache). Ce pipeline convient si on veut une solution 100% Python/OSS intÃ©grable facilement. On peut derriÃ¨re appeler vLLM via son endpoint HTTP ou son client Python. Les performances en FP8 sont excellentes â€“ le blog vLLM rapporte jusquâ€™Ã  2Ã— de gain en latence et 3Ã— en throughput dans certains cas en passant FP16 â†’ FP8
developers.redhat.com
. Il gÃ¨re aussi nativement le PagedAttention. Donc pour un dÃ©ploiement custom (exÂ : dans un script FastAPI), vLLM est tout indiquÃ©.

ğŸ‘‰ RÃ©fÃ©rencesÂ : la documentation de vLLM (readthedocs) et le papier PagedAttention
blog.vllm.ai
blog.vllm.ai
.

Pipeline D â€” llama.cpp / GGUF (Q8_0 / Q4_K_*)

Enfin, pour le prototypage ou lâ€™embarquÃ©, on peut utiliser un export GGUF. Par exemple, on convertit un modÃ¨le en 4-bit GPTQ ou AWQ puis en .gguf via convert-hf-to-gguf.py. Il existe aussi des repos HuggingFace proposant directement des fichiers quantifiÃ©s (exÂ : model-q4_K_M.gguf). Ensuite, on lance le binaire main de llama.cpp ou une UI qui lâ€™utilise. Lâ€™avantage est la simplicitÃ©Â : pas besoin de dÃ©pendances NVIDIA, on peut dÃ©ployer sur un petit serveur CPU ou un Jetson. Les formats de quantization GGUF disponibles incluent Q2_K, Q3_K_M, Q4_0, Q4_K_M, Q5_0, Q5_K_M, Q6_K, Q8_0
qwen.readthedocs.io
. Par exemple, Q8_0 est quasiment du FP16 compressÃ© en 8 bits (peu de perte), Q4_K_M est un 4-bit calibrÃ© Â«Â MediumÂ Â».

Ce pipeline est utile pour partager un modÃ¨le open-source facilementÂ : on fournit juste le .gguf quantifiÃ©, et chacun peut le lancer. Il est aussi prisÃ© pour les dÃ©mos web oÃ¹ le backend peut Ãªtre un CPU cost-efficient. Ã‰videmment, sur H100 on ne va pas utiliser llama.cpp (ce serait gÃ¢cher du potentiel), mais ce pipeline D reste complÃ©mentaire pour dâ€™autres environnements.

10) FAQ rapides

Q1 â€“ Pourquoi tous les modÃ¨les ne â€œtournentâ€ pas en TensorRT-LLM ?
TRT-LLM, bien quâ€™efficace, requiert de supporter explicitement lâ€™architecture du modÃ¨le. Sâ€™il sâ€™agit dâ€™un modÃ¨le Transformer standard (GPT, LLaMA, etc.), câ€™est bon. Mais pour des modÃ¨les avec couches spÃ©ciales ou configurations non conventionnelles, il faut que NVIDIA mette Ã  jour le parser/les kernels. Par exemple, un modÃ¨le qui introduit un nouveau type dâ€™attention ou de feed-forward devra peut-Ãªtre attendre une prise en charge. De plus, TRT-LLM Ã©tant centrÃ© NVIDIA, les auteurs de modÃ¨les open-source ne le considÃ¨rent pas forcÃ©ment comme cible principaleÂ : ils publient en format HF ou GGUF pour la portÃ©e la plus large possible (CPU/AMD/NVIDIA). Il est donc normal que tout nâ€™arrive pas instantanÃ©ment dans TRT-LLM. Toutefois, Ã©tant open-source, la communautÃ© peut contribuer au support de nouveaux modÃ¨les sur TRT-LLM 
developer.nvidia.com
.

Q2 â€“ Puis-je mÃ©langer un modÃ¨le FP8 avec un KV-cache FP16 ?
Oui. Presque toutes les piles permettent de choisir indÃ©pendamment la prÃ©cision des poids/activations et celle du KV-cache. Par exemple, dans vLLM on a --quantization fp8 (pour les poids/acts) et --kv-cache-dtype fp16 si on voulait conserver le KV en haute prÃ©cision. Inversement, on peut faire un modÃ¨le FP16 avec KV en FP8. Ce mix precision peut Ãªtre utile pour peaufiner la qualitÃ© ou Ã©conomiser de la VRAM. Dans nos tests, un modÃ¨le FP8 avec KV en FP16 donne un trÃ¨s lÃ©ger mieux sur des tÃ¢ches trÃ¨s sensibles (exÂ : des puzzles logiques complexes), mais cela double la mÃ©moire KV. Ã€ lâ€™inverse, un modÃ¨le FP16 avec KV en FP8 est presque indiscernable dâ€™un full FP16 sur la plupart des outputs, tout en libÃ©rant pas mal de VRAM. Ã€ vous de voir en fonction de vos contraintes, mais sachez que câ€™est possible (TRT-LLM: flags --use_fp8 vs --use_fp8_kv_cache sÃ©parÃ©s, vLLM idem, etc.).

Q3 â€“ â€œ8-bitâ€ = FP8 ou INT8 ?
Le terme 8-bit peut prÃªter Ã  confusion car il y a deux familles bien distinctesÂ : le FP8 (float 8-bit) et lâ€™INT8 (entier 8-bit). FP8 est une reprÃ©sentation en virgule flottante sur 8 bits, introduite sur H100 (et partiellement disponible sur certaines accÃ©lÃ©rateurs AMD). INT8 est lâ€™approche classique par entiers, supportÃ©e depuis longtemps dans les bibliothÃ¨ques quantization. Les deux visent le mÃªme but (rÃ©duire la prÃ©cision Ã  8 bits), mais fonctionnent diffÃ©remmentÂ : FP8 a une mantisse/exposant, ce qui lui donne une portÃ©e plus flexible (valeurs trÃ¨s petites ou trÃ¨s grandes) pour une mÃªme taille
docs.nvidia.com
, tandis que INT8 a une dynamique fixe mais aucune â€œmagieâ€ dâ€™exposant (il faut bien choisir les Ã©chelles). W8A8 (weights and activations 8-bit) peut dÃ©signer lâ€™un ou lâ€™autre. Sur H100, 8-bit aura tendance Ã  signifier FP8 car câ€™est ce qui donne le meilleur rÃ©sultat. Sur A100 ou dâ€™autres, 8-bit impliquera plutÃ´t INT8 (SmoothQuant ou autre). Il est donc toujours bon de prÃ©ciser.

Q4 â€“ PagedAttention, câ€™est quoi dÃ©jÃ  ?
Câ€™est la technologie de vLLM qui gÃ¨re la mÃ©moire du KV-cache en â€œpagesâ€ plutÃ´t quâ€™en blocs contigus monolithiques. En divisant le KV-cache de chaque requÃªte en petits segments, on peut les allouer et les libÃ©rer de maniÃ¨re flexible, un peu comme la mÃ©moire virtuelle dâ€™un OS
blog.vllm.ai
. Ainsi, on Ã©limine la fragmentation interne/externe (chaque page non utilisÃ©e peut servir ailleurs) et on permet de partager des pages entre requÃªtes (notamment pour le prefix-batching ou le beam search oÃ¹ plusieurs gÃ©nÃ©rations partagent le mÃªme contexte initial)
blog.vllm.ai
. Lâ€™effet est un gaspillage mÃ©moire quasi nul (<4%) et la possibilitÃ© de batcher Ã©normÃ©ment de requÃªtes sans exploser la VRAM. PagedAttention nâ€™a pas dâ€™impact sur la qualitÃ© du modÃ¨le (câ€™est transparent cÃ´tÃ© rÃ©sultats), mais booste le throughput en permettant une meilleure utilisation du GPU
blog.vllm.ai
ar5iv.labs.arxiv.org
. Câ€™est vraiment une avancÃ©e clÃ© pour servir les LLM Ã  grande Ã©chelle.

Q5 â€“ Et les quants GGUF (Q8_0, Q4_K_M, â€¦) dont on voit les nomsâ€¯?
Ce sont les diffÃ©rents presets de quantization utilisÃ©s avec llama.cpp et dâ€™autres outils CPU. En gros, Q8_0 signifie 8 bits non groupÃ© (toutes les matrices quantifiÃ©es globalement, sans offset par groupe), câ€™est la version la plus fidÃ¨le (presque sans perte, on gagne surtout en taille mÃ©moire). Q4_K_M signifie 4 bits, quantization groupÃ©e par blocs de 128 (K), niveau Medium (M) de prÃ©cisionÂ : en pratique Ã§a utilise des Ã©chelles sÃ©parÃ©es par groupes de neurones, ce qui amÃ©liore la fidÃ©litÃ© par rapport Ã  un simple 4-bit homogÃ¨ne. Il existe aussi Q4_0 (4-bit de base), Q4_K_S (4-bit grouped Small), Q5_0, Q5_K_M, etc. La qualitÃ© varie un peu en consÃ©quenceÂ : Q8_0 est trÃ¨s proche du modÃ¨le original, Q4_K_M est lâ€™un des meilleurs compromis en 4-bit, Q4_0 est plus hasardeux (surtout sur modÃ¨les >30B). Pour un GPU H100, ces formats ne tirent pas parti du hardware spÃ©cial (ils seront traitÃ©s comme des INT8 en gros), donc on leur prÃ©fÃ©rera FP8/INT8 via TRT-LLM ou vLLM. En revanche, pour un CPU ou un petit GPU, les quants GGUF sont superÂ : ils permettent de tester rapidement un modÃ¨le sans mobiliser 80Â Go de RAM. On peut par exemple lancer un Llama2 13B Q4_K_M sur un PC 16Â Go RAM â€“ la gÃ©nÃ©ration sera lente, mais Ã§a fonctionne. Donc ces quantizations ont leur place dans lâ€™Ã©cosystÃ¨me, mais ce ne sont pas celles quâ€™on utilisera pour une prod optimale sur H100.

11) Cas particulier : modÃ¨les â€œmergeâ€ et licences

Il existe des modÃ¨les obtenus par merge (fusion de plusieurs checkpoints) qui posent des questions de licence. Par exemple Luminum-123B est un modÃ¨le 123Â milliards rÃ©sultant du merge deÂ : Mistral-Large-Instruct-2407 (base), Lumimaid-v0.2-123B, et Magnum-v2-123B
huggingface.co
huggingface.co
. Chacune de ces composantes a sa propre licenceÂ :

Lumimaid-123B (aussi appelÃ© NeverSleep/Lumimaid-v0.2-123B) est en licence CC-BY-NC-4.0 (Creative Commons Attribution Non-Commercial)
huggingface.co
. Cela signifie usage non commercial uniquement, partage autorisÃ© tant quâ€™on crÃ©dite lâ€™auteur, pas de dÃ©rivÃ©s commerciaux.

Mistral-Large-Instruct-2407 est sous licence Mistral AI Research License (MRL)
huggingface.co
. Câ€™est une licence propriÃ©taire de Mistral AI qui autorise lâ€™usage recherche et le self-hosting non commercial, mais interdit lâ€™usage commercial sans accord explicite. Elle interdit Ã©galement de distribuer les poids dÃ©rivÃ©s Ã  des tiers sans passer par un accord avec MistralÂ AI
huggingface.co
huggingface.co
. En gros, câ€™est non-commercial avec des restrictions supplÃ©mentaires (pas dâ€™exploitation commerciale du modÃ¨le ni de ses dÃ©rivÃ©s sans licence payante).

Magnum 123B quant Ã  lui (si on reprend lâ€™exemple) a probablement une licence du mÃªme acabit (souvent les modÃ¨les Â«Â roleplayÂ Â» sont en Llama2-Community ou autre, on va supposer non-commercial aussi).

En combinant ces modÃ¨les, Luminum hÃ©rite des restrictions les plus fortes de chacun. Autrement dit, Luminum-123B est non-commercial (Ã  cause de Lumimaid CC-BY-NC et Mistral MRL) et ne peut pas Ãªtre distribuÃ© librement en tant que poids merge sans accord (surtout Ã  cause de Mistral MRL qui impose de ne pas partager de dÃ©rivÃ©s). Pour cette raison, lâ€™auteur de Luminum a publiÃ© son modÃ¨le sur HuggingFace mais en marquant quâ€™il faut accepter la MRL pour y accÃ©der, et en rappelant quâ€™il ne faut pas utiliser Ã§a commercialement.

ConsÃ©quence pratiqueÂ : si vous quantifiez un modÃ¨le issu dâ€™un merge sous restriction non-commerciale, vous ne pouvez pas republier les poids quantifiÃ©s (mÃªme en GGUF ou engine TRT) en prÃ©tendant lever la restriction â€“ la quantization ne change pas la licence du contenu. Il faut traiter cela comme un modÃ¨le original pour la licence. Donc, pas de distribution publique de Luminum quantifiÃ© sans autorisation. Ã€ la place, ce quâ€™on peut faire câ€™est partager des instructions de reproduction (par ex. un script de merge + quantization que chacun peut exÃ©cuter de son cÃ´tÃ© aprÃ¨s avoir acceptÃ© les licences sources). On peut aussi Ã©ventuellement distribuer des delta weights ou LoRA si la licence le permet (par ex. Lumimaid Ã©tant open non-commercial, un LoRA dessus reste NC).

En somme, faites bien attention aux licences des modÃ¨les et de leurs donnÃ©es dâ€™entraÃ®nement. Un modÃ¨le comme Llama2 70B base est Llama2-community (autorisation commerciale), mais sa version fine-tunÃ©e par X peut Ãªtre Apache-2.0 ou NC, etc. Toujours vÃ©rifier sur la carte HuggingFaceÂ ! Dans le doute, abstenez-vous de diffuser un dÃ©rivÃ©.

(Exemple rÃ©el : Luminum Ã©tant NC, un utilisateur ne doit pas lâ€™utiliser dans un produit payant. Sâ€™il voulait une version commerciale, il devrait entraÃ®ner ou acquÃ©rir un modÃ¨le Ã©quivalent sous licence permissive. Mistral AI vend une licence pro pour son 7B instruct, par exemple.)

12) Choisir sa quantization (arbre de dÃ©cision)

Pour clÃ´turer, voici un petit guide dÃ©cisionnel pour choisir le bon niveau de quantization selon vos besoinsÂ :

QualitÃ© quasi FP16 + perfs maximales (GPU H100)Â : Optez pour le FP8 (W8A8). Câ€™est idÃ©al si vous avez des H100 ou MI300Â rÃ©centsÂ : vous obtiendrez le meilleur dÃ©bit et des rÃ©ponses presque identiques Ã  FP16. Stacks conseillÃ©esÂ : TensorRT-LLM si vous visez les toutes meilleures latences et un dÃ©ploiement C++ optimisÃ©
developer.nvidia.com
, ou vLLM en FP8 si vous voulez rester en full Python OSS
developers.redhat.com
. Dans les deux cas, activez le KV-cache en FP8 pour bÃ©nÃ©ficier de la mÃ©moire gagnÃ©e.

8-bit â€œclassiqueâ€ toutes plateformesÂ : INT8 SmoothQuant (W8A8). Si vos GPU ne supportent pas FP8 (exÂ : A100) ou si vous tenez Ã  une solution Ã©prouvÃ©e, le combo poids+act en INT8 calibrÃ© est un excellent choix. SmoothQuant a fait ses preuves sur LLM >100B sans perte significative
arxiv.org
. Stacks conseillÃ©esÂ : vLLM --quantization int8, ou des runtimes comme FasterTransformer sur A100 (int8 sans FP8). Nâ€™oubliez pas que INT8 fonctionne aussi bien sur CPU (on commence Ã  voir des accÃ©lÃ©rations int8 sur CPU via ONNXRuntime par ex).

Compression agressive / VRAM limitÃ©eÂ : INT4 (AWQ/GPTQ). Si vous devez faire tenir un modÃ¨le trÃ¨s gros dans peu de mÃ©moire, ou lancer plein dâ€™instances parallÃ¨les, le 4-bit weight-only est la solution. Vous sacrifierez un peu de â€œhumanitÃ©â€ dans les rÃ©ponses (phrases un peu plus gÃ©nÃ©riques, style moins raffinÃ©), mais le modÃ¨le restera fonctionnel pour de nombreuses tÃ¢ches. Stack conseillÃ©eÂ : llama.cpp GGUF Q4_K_M ou AutoGPTQ (pour avoir un modÃ¨le 4-bit utilisable dans Transformers sur GPU). Sur H100, vous pouvez aussi combiner un modÃ¨le 4-bit avec un KV-cache FP8 via TensorRT-LLM (ils ont montrÃ© Falcon-180B en INT4 AWQ tournant sur un seul H200 dans un de leurs blogs!).

Prototypage rapide / EdgeÂ : GGUF (Q8_0, Q4_K, etc.) via llama.cpp. Si votre but est de tester un modÃ¨le en local, ou de le dÃ©ployer sur une machine sans GPU NVIDIA, partez sur les quantizations fournies par la communautÃ© en GGUF. Ã‡a Ã©vite tout tracas dâ€™installation et Ã§a marche out of the box. La qualitÃ© dÃ©pend du preset (prendre de prÃ©fÃ©rence les versions â€œK_Mâ€ en 4/5 bits pour un bon Ã©quilibre qualitÃ©). Nâ€™espÃ©rez pas la mÃªme vitesse quâ€™avec un GPU pro, mais pour des dÃ©mos ou du dev câ€™est suffisant.

(En cas de doute, commencez par du FP16 ou FP8, voyez si la latence/mÃ©moire vous conviennent, puis descendez dâ€™un cran si nÃ©cessaire. Mieux vaut une rÃ©ponse un peu lente mais fiable, quâ€™un modÃ¨le compressÃ© Ã  outrance mais dÃ©cevant.)

13) Commandes types (rÃ©fÃ©rence rapide)

Voici un rÃ©capitulatif de quelques commandes Ã©voquÃ©es, pour rÃ©fÃ©renceÂ :

A) TensorRT-LLM (H100, FP8) â€“ Exporter un modÃ¨le HF et builder en FP8Â :

# Export HF -> TRT-LLM checkpoint
python examples/llama/convert_checkpoint.py \
   --model_dir /chemin/vers/modele_hf \
   --output_dir /chemin/vers/output_trtllm_ckpt \
   --dtype float16 --tp_size 2   # si multi-GPU

# Build engine FP8 + KV FP8
trtllm-build \
   --checkpoint_dir /chemin/vers/output_trtllm_ckpt \
   --output_dir /chemin/vers/engine_fp8 \
   --use_fp8 --use_fp8_kv_cache \
   --max_batch_size 8 \
   --max_input_len 8192 --max_output_len 1024 \
   --tp_size 2  # si multi-GPU

# Serveur TRT-LLM
trtllm-serve --engine_dir /chemin/vers/engine_fp8 --port 8080


(Cf. docs TRT-LLM pour plus de dÃ©tails
developer.nvidia.com
. Pensez Ã  ajuster batch_size et lengths Ã  vos besoins rÃ©els pour optimiser la compilation.)

B) vLLM FP8 (W8A8 + KV FP8) â€“ Lancer un serveur vLLM quantifiÃ© 8-bitÂ :

vllm serve ORGANISATION/MODELE-HF \
   --quantization fp8 \
   --kv-cache-dtype fp8 \
   --max-model-len 16384


(NÃ©cessite GPU H100 ou matÃ©riel supportant FP8. Cf. vLLM docs
developers.redhat.com
.)

C) vLLM INT8 (W8A8 + KV FP8) â€“ Lancer vLLM en SmoothQuant 8-bitÂ :

vllm serve ORGANISATION/MODELE-HF \
   --quantization int8 \
   --kv-cache-dtype fp8 \
   --max-model-len 16384


(Fonctionne sur A100/H100. Si pas de FP8 du tout, mettre kv-cache-dtype Ã  fp16. On peut aussi charger un modÃ¨le AWQ en passant --quantization awq et en pointant vers le fichier .pt quantifiÃ©.)

D) Conversion GGUF (llama.cpp) â€“ Convertir et quantifier un modÃ¨le en GGUFÂ :

# 1. Convertir un modÃ¨le HF en GGUF FP16
python convert-hf-to-gguf.py NomDuModeleHF --outfile modele.gguf

# 2. Quantifier en 4 bits par ex.
./quantize modele.gguf modele-q4_0.gguf q4_0


(Voir documentation Qwen/llama.cpp
qwen.readthedocs.io
qwen.readthedocs.io
. Il existe aussi des scripts pour appliquer AWQ avant conversion afin dâ€™amÃ©liorer la qualitÃ© comme vu plus haut.)

14) Points de contrÃ´le (qualitÃ©)

Avant de dÃ©ployer en production, pensez Ã  passer votre modÃ¨le quantifiÃ© par quelques points de contrÃ´le qualitÃ©Â :

Jeu de validationÂ : PrÃ©parez un set de prompts variÃ©s (10-50, selon vos ressources), couvrant les cas dâ€™usage typiques. IdÃ©alement multi-langues si concernÃ©. Incluez des conversations multi-tours, des questions piÃ¨ges, des demandes de gÃ©nÃ©ration crÃ©ative, etc. Faites gÃ©nÃ©rer le modÃ¨le FP16 et le modÃ¨le quantifiÃ© sur ces prompts, et comparez. Cherchez les diffÃ©rences flagrantes (rÃ©pÃ©titions, ignorances dâ€™instructions, rÃ©ponses Ã  cÃ´tÃ©â€¦).

MÃ©triques autoÂ : Si possible, Ã©valuez la perplexitÃ© du modÃ¨le quantifiÃ© sur un corpus de test. Un Ã©cart de perplexitÃ© trÃ¨s faible (<5-10%) par rapport au FP16 est bon signe. Vous pouvez aussi calculer des mÃ©triques de diversitÃ© lexicale comme distinct-n sur des longues gÃ©nÃ©rationsÂ : un modÃ¨le quantifiÃ© de faÃ§on agressive a parfois tendance Ã  recycler les mÃªmes tournures, ce qui rÃ©duit distinct-4/5. Enfin, si votre application craint les hallucinations ou les refus injustifiÃ©s, testez-en quelques-uns (exÂ : demandes factuelles pour voir si le quant hallucine plusÂ ; requÃªtes sensibles pour voir sâ€™il se met Ã  refuser inutilement).

A/B testingÂ : Le mieux reste de faire Ã©valuer quelques paires de rÃ©ponses (FP16 vs quant) par des humains sans leur dire qui est qui. Sâ€™ils nâ€™y voient que du feu ou prÃ©fÃ¨rent mÃªme parfois la version quantifiÃ©e, câ€™est gagnÃ© ğŸ™‚.

RÃ©glage des hyperparamÃ¨tresÂ : Un modÃ¨le quantifiÃ© peut nÃ©cessiter de lÃ©gers ajustements de sampling. En particulier, augmenter le repetition_penalty (p.ex. de 1.1 Ã  1.15) peut aider Ã  garder le style cohÃ©rent sur de longues rÃ©ponses. On peut aussi ajuster le top_p ou temperature si on constate des sorties moins variÃ©es. Nâ€™hÃ©sitez pas Ã  tuner ces paramÃ¨tres sur votre set de validation. Parfois, un quant de 4-bit apprÃ©ciera une tempÃ©rature un poil plus Ã©levÃ©e pour compenser la perte de finesse.

Long contexteÂ : Si vous visez du 16k ou 32k tokens, testez-leÂ ! Envoyez un prompt de ~15k tokens et voyez si le modÃ¨le continue correctement. Sur de trÃ¨s longs contextes, la quantization peut accumuler de lâ€™erreur numÃ©rique (dâ€™oÃ¹ lâ€™intÃ©rÃªt du KV en FP8 ou FP16). Assurez-vous que la dÃ©gradation reste gÃ©rable (de toute faÃ§on, au-delÃ  de 8k mÃªme un modÃ¨le FP16 commence Ã  flancher parfois).

En suivant ces points de contrÃ´le, vous aurez lâ€™assurance que votre modÃ¨le quantifiÃ© tient la route. La quantization est un art subtilÂ : 99% du temps Ã§a marche trÃ¨s bien, mais il vaut mieux dÃ©busquer le 1% de cas oÃ¹ Ã§a pourrait poser souci avant que les utilisateurs ne tombent dessus.

15) TL;DR

H100 = FP8 natif ğŸ“ˆÂ : Les GPU NVIDIA Hopper (H100) supportent nativement le calcul en float8 via la Transformer Engine. Cela permet dâ€™atteindre des performances jusquâ€™Ã  ~4â€“5Ã— supÃ©rieures Ã  A100 FP16, avec une qualitÃ© de modÃ¨le pratiquement inchangÃ©e si calibrÃ© correctement
nvidia.github.io
developers.redhat.com
. En clair, FP8 sur H100 offre le meilleur ratio qualitÃ©/latence/VRAM aujourdâ€™hui.

TensorRT-LLM ğŸš€Â : Câ€™est la solution NVIDIA optimisÃ©e pour infÃ©rence LLM. Elle compile le modÃ¨le en un engine ultra-rapide. AvantagesÂ : support du FP8 et INT8 (SmoothQuant) directement, batching asynchrone en vol, KV-cache paginÃ©, multi-GPUâ€¦ Bref, câ€™est ce qui donnera les latences et throughputs minimum sur H100
developer.nvidia.com
. InconvÃ©nientÂ : spÃ©cifique NVIDIA, et nÃ©cessite de passer par une Ã©tape de build.

vLLM ğŸÂ : Serveur haute performance open-source. Il introduit PagedAttention qui rÃ©duit le gÃ¢chis mÃ©moire du KV-cache Ã  <4%, permettant de booster le throughput sans changer de hardware
runpod.io
. vLLM supporte aussi FP8 et INT8 (ainsi que chargement de modÃ¨les 4-bit). IdÃ©al si on veut une intÃ©gration simple (quelques lignes Python) tout en gardant des perfs state-of-the-art. Câ€™est open-source (Apache 2.0). Moins rapide que TRT-LLM sur un seul GPU, mais plus flexible.

Choix de quantization ğŸ¤–Â :

Pour la qualitÃ© maxÂ : FP8 (8-bit flottant) si possible, sinon INT8 SmoothQuant. Ces deux options donnent des rÃ©sultats quasi identiques au FP16 original sur la plupart des modÃ¨les
arxiv.org
.

Pour pousser la compressionÂ : INT4 (4-bit poids) via AWQ/GPTQ est faisable sur des grands modÃ¨les, au prix dâ€™une trÃ¨s lÃ©gÃ¨re dÃ©gradation du style/cohÃ©rence. Ã€ utiliser si VRAM limitÃ©e ou pour hÃ©berger plusieurs instances.

Le tout sans rÃ©entraÃ®ner (PTQ). On peut quantizer un modÃ¨le aprÃ¨s-coup et le servir directement.

Formats GGUF (llama.cpp) ğŸ’¾Â : Utiles pour exÃ©cuter des LLM sur CPU ou petits GPU. ExemplesÂ : Q8_0 (8-bit poids), Q4_K_M (4-bit groupe Medium)
medium.com
. Ils rendent les modÃ¨les plus accessibles, au prix dâ€™une vitesse moindre. Sur H100, ces formats ne tirent pas profit du hardware spÃ©cialisÃ©, donc on privilÃ©giera plutÃ´t TRT-LLM/vLLM. Mais pour du offline ou du local sans CUDA, câ€™est gÃ©nial.

Licences & modÃ¨les merges ğŸ“œÂ : Attention Ã  la lÃ©galitÃ©Â ! Un modÃ¨le comme Luminum-123B mergeant Mistral (licence MRL, non-commercial) et Lumimaid (CC-BY-NC-4.0) reste Non-Commercial et soumis aux restrictions de diffusion des originaux
huggingface.co
huggingface.co
. Quantizer un modÃ¨le ne change pas sa licence. Il est gÃ©nÃ©ralement interdit de redistribuer des poids dÃ©rivÃ©s sans accord si la licence source lâ€™interdit (exÂ : Mistral MRL prohibe de partager le modÃ¨le fine-tunÃ© sans passer par eux
huggingface.co
). PrÃ©fÃ©rez partager des scripts ou des diffs/LoRA plutÃ´t que les poids quantifiÃ©s directement pour ces cas. En clairÂ : toujours respecter les licences, mÃªme pour un modÃ¨le quantifiÃ© ou compressÃ©Â !
